{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 07-08: Linear Regression & Gradient Descent\n",
        "## Possible Subjective Exam Questions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Introduction and Motivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. Why is accurate forecasting important for renewable energy generation? Explain with an example.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Accurate forecasting is important because:\n",
        "\n",
        "**Problem with Renewable Energy:**\n",
        "1. Wind and solar power are intermittent (not always available)\n",
        "2. When wind stops, we need backup from traditional power plants\n",
        "3. Traditional plants (like nuclear) cannot easily be turned on/off quickly\n",
        "4. They cannot be ramped up or down easily\n",
        "\n",
        "**Why Forecasting Helps:**\n",
        "1. If we know when wind will stop, we can prepare backup in advance\n",
        "2. Makes renewable energy more efficient and reliable\n",
        "3. Saves money by better planning\n",
        "\n",
        "**Real Example:**\n",
        "Wind forecasting algorithms (using Machine Learning) at NCAR saved utility companies \\$6-\\$10 million per year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. Why is it difficult to estimate energy consumption using \"a priori\" models? How does data-driven approach help?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Why A Priori Models are Difficult:**\n",
        "1. Energy consumption depends on many factors\n",
        "2. Hard to write equations that capture all relationships\n",
        "3. Human behavior is unpredictable\n",
        "4. Weather patterns are complex\n",
        "\n",
        "**How Data-Driven Approach Helps:**\n",
        "1. We have abundant historical data available\n",
        "2. We can learn patterns from past data\n",
        "3. The model discovers relationships automatically\n",
        "4. No need to manually define all rules\n",
        "5. Model improves as more data becomes available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. What factors affect electricity consumption? How can visualization help in understanding these patterns?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Factors Affecting Electricity Consumption:**\n",
        "\n",
        "1. **Hour of the day:** Consumption varies throughout the day\n",
        "2. **Season:** Different consumption in February vs July vs October\n",
        "3. **Temperature:** Higher temperature leads to more AC usage\n",
        "4. **Day of week:** Weekdays vs weekends have different patterns\n",
        "\n",
        "**How Visualization Helps:**\n",
        "1. Shows correlation between variables\n",
        "2. Reveals patterns that are not obvious\n",
        "3. Helps identify which features are useful for prediction\n",
        "4. Example: Plotting Peak Hourly Demand (GW) vs High Temperature (F) shows a clear relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Linear Model Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. Write the general formula for a linear model in simple regression. Explain each component.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**General Formula:**\n",
        "\n",
        "$$\\text{predicted peak demand} = \\theta_1 \\cdot (\\text{high temperature}) + \\theta_2$$\n",
        "\n",
        "**Components Explained:**\n",
        "\n",
        "| Component | Meaning |\n",
        "|-----------|----------|\n",
        "| $\\theta_1$ | Slope - how much output changes when input changes by 1 unit |\n",
        "| $\\theta_2$ | Intercept - the output value when input is zero |\n",
        "| high temperature | Input feature (independent variable) |\n",
        "| predicted peak demand | Output (dependent variable) |\n",
        "\n",
        "**Note:** Both $\\theta_1$ and $\\theta_2$ are real numbers ($\\theta_1, \\theta_2 \\in \\mathbb{R}$) learned from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. Given $\\theta_1 = 0.046$ and $\\theta_2 = -1.46$, calculate the predicted peak demand when the high temperature is 80°F. Show your work.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:**\n",
        "- $\\theta_1 = 0.046$\n",
        "- $\\theta_2 = -1.46$\n",
        "- High temperature = 80°F\n",
        "\n",
        "**Formula:**\n",
        "$$\\text{predicted peak demand} = \\theta_1 \\cdot (\\text{temperature}) + \\theta_2$$\n",
        "\n",
        "**Calculation:**\n",
        "$$\\text{predicted peak demand} = 0.046 \\times 80 + (-1.46)$$\n",
        "\n",
        "$$= 3.68 - 1.46$$\n",
        "\n",
        "$$= 2.22 \\text{ GW}$$\n",
        "\n",
        "**Answer:** The predicted peak demand is approximately **2.22 GW** (or 2.19 GW as given in slides)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. What is the Least Squares Method? Why is it used in linear regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Least Squares Method:**\n",
        "A method to find the best-fitting line by minimizing the sum of squared differences between predicted and actual values.\n",
        "\n",
        "**Why It's Used:**\n",
        "\n",
        "1. **Minimizes error:** Finds the line that is closest to all data points\n",
        "\n",
        "2. **Squared errors:** Using squares makes all errors positive and penalizes large errors more\n",
        "\n",
        "3. **Mathematical convenience:** Leads to a nice analytical solution\n",
        "\n",
        "4. **Unique solution:** Always gives one best answer\n",
        "\n",
        "**Goal:** Find $\\theta$ values that minimize:\n",
        "$$\\sum_{i=1}^{m} (\\text{predicted}_i - \\text{actual}_i)^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Fitting a Line - Manual Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. Explain the step-by-step process to fit a line to 2D data points.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Steps to Fit a Line:**\n",
        "\n",
        "**Step 1:** Calculate the mean of X values\n",
        "$$\\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n",
        "\n",
        "**Step 2:** Calculate the mean of Y values\n",
        "$$\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$\n",
        "\n",
        "**Step 3:** Calculate the slope (m) using:\n",
        "$$m = \\frac{\\sum_{i=1}^{n}(x_i - \\overline{X})(y_i - \\overline{Y})}{\\sum_{i=1}^{n}(x_i - \\overline{X})^2}$$\n",
        "\n",
        "**Step 4:** Calculate the intercept (b) using:\n",
        "$$b = \\overline{Y} - m \\cdot \\overline{X}$$\n",
        "\n",
        "**Step 5:** Write the final equation:\n",
        "$$y = mx + b$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. Derive and explain the formula for calculating slope in linear regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Slope Formula:**\n",
        "\n",
        "$$m = \\frac{\\sum_{i=1}^{n}(x_i - \\overline{X})(y_i - \\overline{Y})}{\\sum_{i=1}^{n}(x_i - \\overline{X})^2}$$\n",
        "\n",
        "**Explanation of Components:**\n",
        "\n",
        "**Numerator:** $\\sum_{i=1}^{n}(x_i - \\overline{X})(y_i - \\overline{Y})$\n",
        "- This is the covariance between X and Y\n",
        "- Measures how X and Y change together\n",
        "- Positive if they increase together\n",
        "\n",
        "**Denominator:** $\\sum_{i=1}^{n}(x_i - \\overline{X})^2$\n",
        "- This is the variance of X\n",
        "- Measures how spread out X values are\n",
        "\n",
        "**Meaning of Slope:**\n",
        "- How much Y changes when X increases by 1 unit\n",
        "- Positive slope: Y increases as X increases\n",
        "- Negative slope: Y decreases as X increases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. Why do we subtract the mean ($\\overline{X}$ and $\\overline{Y}$) in the slope formula?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Reasons for Subtracting Mean:**\n",
        "\n",
        "1. **Centering the data:** Moves the data so that the center is at origin (0,0)\n",
        "\n",
        "2. **Removes bias:** Focuses on the relationship, not the absolute values\n",
        "\n",
        "3. **Measures deviation:** Shows how each point differs from the average\n",
        "\n",
        "4. **Covariance calculation:** The formula essentially calculates covariance divided by variance\n",
        "\n",
        "5. **Ensures line passes through mean point:** The regression line always passes through $(\\overline{X}, \\overline{Y})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. Given slope $m = -1.1$ and intercept $b = 14.0$, interpret what these values mean in context of a prediction problem.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Final Equation:** $y = -1.1x + 14.0$\n",
        "\n",
        "**Interpretation of Slope ($m = -1.1$):**\n",
        "- Negative slope indicates inverse relationship\n",
        "- For every 1 unit increase in X, Y decreases by 1.1 units\n",
        "- Example: If temperature increases by 1°F, demand might decrease by 1.1 units\n",
        "\n",
        "**Interpretation of Intercept ($b = 14.0$):**\n",
        "- When X = 0, the predicted Y = 14.0\n",
        "- This is the baseline value\n",
        "- May or may not have practical meaning depending on context\n",
        "\n",
        "**Using the Model:**\n",
        "- If $x = 5$: $y = -1.1(5) + 14.0 = -5.5 + 14.0 = 8.5$\n",
        "- If $x = 10$: $y = -1.1(10) + 14.0 = -11 + 14.0 = 3.0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Formal Problem Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11. Define the formal notation used in linear regression. Explain input, output, parameters, and feature mapping.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Formal Notation:**\n",
        "\n",
        "| Symbol | Meaning | Example |\n",
        "|--------|---------|----------|\n",
        "| $x_i \\in \\mathbb{R}^n$ | Input vector for sample $i$ | High temperature for day $i$ |\n",
        "| $y_i \\in \\mathbb{R}$ | Output (target) for sample $i$ | Peak demand for day $i$ |\n",
        "| $\\theta \\in \\mathbb{R}^k$ | Model parameters to learn | Slope and intercept |\n",
        "| $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k$ | Feature mapping function | Maps input to feature vector |\n",
        "\n",
        "**Feature Mapping Example:**\n",
        "- For $n=1$ (one input) and $k=2$ (two parameters)\n",
        "- $\\phi(x_i) = [x_i, 1]^T$\n",
        "- This adds a 1 for the intercept term\n",
        "\n",
        "**This is a Regression Task:** Output is a continuous real number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12. What is feature mapping? Why do we add a \"1\" to the feature vector?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Feature Mapping ($\\phi$):**\n",
        "A function that transforms the input into a feature vector suitable for the model.\n",
        "\n",
        "**Example:**\n",
        "$$\\phi(x_i) = [x_i, 1]^T$$\n",
        "\n",
        "**Why Add \"1\":**\n",
        "\n",
        "1. **For the intercept term:** The \"1\" allows us to include a bias/intercept in our model\n",
        "\n",
        "2. **Compact notation:** Prediction becomes a simple dot product:\n",
        "$$\\hat{y}_i = \\theta^T \\phi(x_i) = [\\theta_1, \\theta_2] \\cdot [x_i, 1]^T = \\theta_1 x_i + \\theta_2$$\n",
        "\n",
        "3. **Uniform treatment:** Both slope and intercept are treated as parameters\n",
        "\n",
        "4. **Matrix operations:** Makes it easy to use matrix notation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13. Write and explain the prediction formula in linear regression using vector notation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Prediction Formula:**\n",
        "\n",
        "$$\\hat{y}_i = \\sum_{j=1}^{k} \\theta_j \\cdot \\phi_j(x_i) \\equiv \\theta^T \\phi(x_i)$$\n",
        "\n",
        "**Expanded Form:**\n",
        "$$\\hat{y}_i = \\theta_1 \\cdot \\phi_1(x_i) + \\theta_2 \\cdot \\phi_2(x_i) + ... + \\theta_k \\cdot \\phi_k(x_i)$$\n",
        "\n",
        "**Components:**\n",
        "- $\\hat{y}_i$ = Predicted output for sample $i$\n",
        "- $\\theta_j$ = The $j$-th parameter\n",
        "- $\\phi_j(x_i)$ = The $j$-th feature of input $x_i$\n",
        "- $\\theta^T \\phi(x_i)$ = Dot product of parameter vector and feature vector\n",
        "\n",
        "**Example with $k=2$:**\n",
        "$$\\hat{y}_i = \\theta_1 \\cdot x_i + \\theta_2 \\cdot 1 = \\theta_1 x_i + \\theta_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14. What is a loss function? Why do we use squared loss in linear regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Loss Function:**\n",
        "A function that measures how \"close\" the predicted value is to the actual value.\n",
        "\n",
        "$$l: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}_+$$\n",
        "\n",
        "It takes predicted and actual values and returns a non-negative number.\n",
        "\n",
        "**Squared Loss:**\n",
        "$$l(\\hat{y}_i, y_i) = (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "**Why Use Squared Loss:**\n",
        "\n",
        "1. **Always positive:** Squaring ensures loss is never negative\n",
        "\n",
        "2. **Penalizes large errors more:** An error of 10 costs 100, not just 10\n",
        "\n",
        "3. **Differentiable:** Easy to compute gradients for optimization\n",
        "\n",
        "4. **Analytical solution:** Leads to closed-form solution (Normal Equations)\n",
        "\n",
        "5. **Statistical interpretation:** Optimal under Gaussian noise assumption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15. Write and explain the cost function (objective function) for linear regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Cost Function:**\n",
        "\n",
        "$$J(\\theta) = \\sum_{i=1}^{m} (\\theta^T \\phi(x_i) - y_i)^2$$\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "| Part | Meaning |\n",
        "|------|----------|\n",
        "| $J(\\theta)$ | Total cost as a function of parameters |\n",
        "| $m$ | Total number of training samples |\n",
        "| $\\theta^T \\phi(x_i)$ | Predicted value $\\hat{y}_i$ |\n",
        "| $y_i$ | Actual/true value |\n",
        "| $(\\theta^T \\phi(x_i) - y_i)^2$ | Squared error for sample $i$ |\n",
        "| $\\sum_{i=1}^{m}$ | Sum over all samples |\n",
        "\n",
        "**Objective:** We want to find $\\theta$ that minimizes $J(\\theta)$\n",
        "\n",
        "**Goal:** Find $\\theta^*$ such that $\\hat{y}_i \\approx y_i$ for all samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q16. What is the difference between loss function and cost function?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Loss Function | Cost Function |\n",
        "|---------------|---------------|\n",
        "| For a single sample | For all samples combined |\n",
        "| $l(\\hat{y}_i, y_i) = (\\hat{y}_i - y_i)^2$ | $J(\\theta) = \\sum_{i=1}^{m} l(\\hat{y}_i, y_i)$ |\n",
        "| Measures individual error | Measures total/average error |\n",
        "| Also called error function | Also called objective function |\n",
        "\n",
        "**Relationship:**\n",
        "$$\\text{Cost Function} = \\sum (\\text{Loss Function for each sample})$$\n",
        "\n",
        "Sometimes cost function includes average:\n",
        "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17. Derive the partial derivative of the cost function with respect to $\\theta_j$.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Cost Function:**\n",
        "$$J(\\theta) = \\sum_{i=1}^{m} (\\theta^T \\phi(x_i) - y_i)^2$$\n",
        "\n",
        "**Derivation:**\n",
        "\n",
        "Using chain rule:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\sum_{i=1}^{m} (\\theta^T \\phi(x_i) - y_i)^2$$\n",
        "\n",
        "$$= \\sum_{i=1}^{m} 2(\\theta^T \\phi(x_i) - y_i) \\cdot \\frac{\\partial}{\\partial \\theta_j}(\\theta^T \\phi(x_i) - y_i)$$\n",
        "\n",
        "Since $\\frac{\\partial}{\\partial \\theta_j}(\\theta^T \\phi(x_i)) = \\phi_j(x_i)$:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=1}^{m} 2(\\theta^T \\phi(x_i) - y_i) \\cdot \\phi_j(x_i)$$\n",
        "\n",
        "**Or simply:**\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = 2 \\sum_{i=1}^{m} (\\hat{y}_i - y_i) \\cdot \\phi_j(x_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18. What is the Design Matrix? How is it constructed?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Design Matrix ($\\Phi$):**\n",
        "A matrix that contains all the feature vectors for all samples.\n",
        "\n",
        "**Construction:**\n",
        "\n",
        "$$\\Phi = \\begin{bmatrix} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_m)^T \\end{bmatrix}$$\n",
        "\n",
        "**Dimensions:** $\\Phi \\in \\mathbb{R}^{m \\times k}$\n",
        "- $m$ = number of samples (rows)\n",
        "- $k$ = number of features (columns)\n",
        "\n",
        "**Example:**\n",
        "If $\\phi(x_i) = [x_i, 1]^T$ for 3 samples:\n",
        "\n",
        "$$\\Phi = \\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\\\ x_3 & 1 \\end{bmatrix}$$\n",
        "\n",
        "**Target Vector:** $y \\in \\mathbb{R}^m = [y_1, y_2, ..., y_m]^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19. Write the least-squares objective in matrix notation and explain each term.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Matrix Form of Cost Function:**\n",
        "\n",
        "$$J(\\theta) = ||\\Phi\\theta - y||_2^2$$\n",
        "\n",
        "**Explanation of Terms:**\n",
        "\n",
        "| Term | Meaning | Dimension |\n",
        "|------|---------|------------|\n",
        "| $\\Phi$ | Design matrix | $m \\times k$ |\n",
        "| $\\theta$ | Parameter vector | $k \\times 1$ |\n",
        "| $y$ | Target vector | $m \\times 1$ |\n",
        "| $\\Phi\\theta$ | Predictions for all samples | $m \\times 1$ |\n",
        "| $\\Phi\\theta - y$ | Error vector | $m \\times 1$ |\n",
        "| $||\\cdot||_2^2$ | Squared L2 norm | scalar |\n",
        "\n",
        "**Interpretation:**\n",
        "This measures the total squared difference between predictions and actual values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20. What is the condition for finding the minimum of a function? How does it differ for 1D vs multi-variate case?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**1-D Case (single variable):**\n",
        "\n",
        "The minimum is found where the derivative equals zero:\n",
        "$$\\frac{dJ}{d\\theta} = 0$$\n",
        "\n",
        "**Multi-variate Case ($\\theta \\in \\mathbb{R}^k$):**\n",
        "\n",
        "The minimum is found where the gradient (all partial derivatives) equals zero:\n",
        "$$\\nabla_{\\theta} J(\\theta) = 0$$\n",
        "\n",
        "Where the gradient is:\n",
        "$$\\nabla_{\\theta} J(\\theta) = \\begin{bmatrix} \\frac{\\partial J}{\\partial \\theta_1} \\\\ \\frac{\\partial J}{\\partial \\theta_2} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial \\theta_k} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$$\n",
        "\n",
        "This gives us $k$ equations to solve for $k$ unknowns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Normal Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21. Expand the least-squares objective function and show the detailed matrix form.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Starting Point:**\n",
        "$$J(\\theta) = ||\\Phi\\theta - y||_2^2$$\n",
        "\n",
        "**Expansion:**\n",
        "$$J(\\theta) = (\\Phi\\theta - y)^T (\\Phi\\theta - y)$$\n",
        "\n",
        "Using $(A - B)^T = A^T - B^T$:\n",
        "$$= (\\theta^T\\Phi^T - y^T)(\\Phi\\theta - y)$$\n",
        "\n",
        "Expanding:\n",
        "$$= \\theta^T\\Phi^T\\Phi\\theta - \\theta^T\\Phi^Ty - y^T\\Phi\\theta + y^Ty$$\n",
        "\n",
        "Since $\\theta^T\\Phi^Ty$ and $y^T\\Phi\\theta$ are scalars and equal:\n",
        "$$J(\\theta) = \\theta^T\\Phi^T\\Phi\\theta - 2y^T\\Phi\\theta + y^Ty$$\n",
        "\n",
        "**Terms:**\n",
        "- $\\theta^T\\Phi^T\\Phi\\theta$: Quadratic term in $\\theta$\n",
        "- $-2y^T\\Phi\\theta$: Linear term in $\\theta$\n",
        "- $y^Ty$: Constant (doesn't affect optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22. Compute the gradient of the cost function and derive the Normal Equations.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Cost Function:**\n",
        "$$J(\\theta) = \\theta^T\\Phi^T\\Phi\\theta - 2y^T\\Phi\\theta + y^Ty$$\n",
        "\n",
        "**Computing Gradient:**\n",
        "\n",
        "Using matrix calculus rules:\n",
        "- $\\nabla_{\\theta}(\\theta^T A \\theta) = 2A\\theta$ (for symmetric $A$)\n",
        "- $\\nabla_{\\theta}(b^T\\theta) = b$\n",
        "\n",
        "$$\\nabla_{\\theta} J(\\theta) = 2\\Phi^T\\Phi\\theta - 2\\Phi^Ty$$\n",
        "\n",
        "**Setting Gradient to Zero:**\n",
        "$$2\\Phi^T\\Phi\\theta - 2\\Phi^Ty = 0$$\n",
        "\n",
        "$$\\Phi^T\\Phi\\theta = \\Phi^Ty$$\n",
        "\n",
        "**Normal Equations Solution:**\n",
        "$$\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
        "\n",
        "**This is the analytical (closed-form) solution!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23. State and explain the Normal Equations. Why is this solution called \"analytical\"?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Normal Equations:**\n",
        "\n",
        "$$\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
        "\n",
        "**Explanation of Each Part:**\n",
        "\n",
        "| Term | Dimension | Meaning |\n",
        "|------|-----------|----------|\n",
        "| $\\Phi^T$ | $k \\times m$ | Transpose of design matrix |\n",
        "| $\\Phi^T\\Phi$ | $k \\times k$ | Gram matrix (symmetric) |\n",
        "| $(\\Phi^T\\Phi)^{-1}$ | $k \\times k$ | Inverse of Gram matrix |\n",
        "| $\\Phi^Ty$ | $k \\times 1$ | Correlation between features and target |\n",
        "| $\\theta^*$ | $k \\times 1$ | Optimal parameters |\n",
        "\n",
        "**Why \"Analytical\":**\n",
        "1. We can directly compute the exact solution\n",
        "2. No iterative algorithm needed\n",
        "3. One-shot computation\n",
        "4. This is rare - most ML problems don't have closed-form solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24. What are the conditions for the Normal Equations solution to exist?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Condition:** The matrix $(\\Phi^T\\Phi)$ must be invertible.\n",
        "\n",
        "**When is it invertible:**\n",
        "\n",
        "1. **Full column rank:** $\\Phi$ must have linearly independent columns\n",
        "2. **Enough samples:** Number of samples $m$ must be at least $k$ (number of features)\n",
        "3. **No redundant features:** Features should not be perfectly correlated\n",
        "\n",
        "**Problems when not invertible:**\n",
        "1. Too many features compared to samples\n",
        "2. Multicollinearity (features are highly correlated)\n",
        "3. Duplicate features\n",
        "\n",
        "**Solutions:**\n",
        "1. Use pseudo-inverse instead\n",
        "2. Add regularization (Ridge regression)\n",
        "3. Remove redundant features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Multidimensional Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25. How do we extend linear regression to handle multiple input features?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Scenario:** Input $x \\in \\mathbb{R}^2$ (e.g., Temperature and Hour of Day)\n",
        "\n",
        "**Feature Vector:**\n",
        "$$\\phi(x) = [\\text{temperature}, \\text{hour of day}, 1]^T \\in \\mathbb{R}^3$$\n",
        "\n",
        "**Design Matrix:**\n",
        "$$\\Phi = \\begin{bmatrix} \\text{temp}_1 & \\text{hour}_1 & 1 \\\\ \\text{temp}_2 & \\text{hour}_2 & 1 \\\\ \\vdots & \\vdots & \\vdots \\\\ \\text{temp}_m & \\text{hour}_m & 1 \\end{bmatrix} \\in \\mathbb{R}^{m \\times 3}$$\n",
        "\n",
        "**Solution remains the same:**\n",
        "$$\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
        "\n",
        "**Prediction:**\n",
        "$$\\hat{y} = \\theta_1 \\cdot \\text{temperature} + \\theta_2 \\cdot \\text{hour} + \\theta_3$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26. Write the general formula for linear regression with $n$ input features.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**General Setting:**\n",
        "- Input: $x \\in \\mathbb{R}^n$ with $n$ features\n",
        "- Feature vector: $\\phi(x) \\in \\mathbb{R}^{n+1}$ (adding 1 for intercept)\n",
        "- Parameters: $\\theta \\in \\mathbb{R}^{n+1}$\n",
        "\n",
        "**Feature Mapping:**\n",
        "$$\\phi(x) = [x_1, x_2, ..., x_n, 1]^T$$\n",
        "\n",
        "**Prediction Formula:**\n",
        "$$\\hat{y} = \\theta^T\\phi(x) = \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n + \\theta_{n+1}$$\n",
        "\n",
        "**Or in summation form:**\n",
        "$$\\hat{y} = \\sum_{j=1}^{n} \\theta_j x_j + \\theta_{n+1}$$\n",
        "\n",
        "**Optimal Parameters:**\n",
        "$$\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Numerical Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27. Given the following data points, calculate the slope and intercept of the regression line.\n",
        "\n",
        "| X | Y |\n",
        "|---|---|\n",
        "| 1 | 3 |\n",
        "| 2 | 5 |\n",
        "| 3 | 7 |\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Step 1: Calculate means**\n",
        "$$\\overline{X} = \\frac{1+2+3}{3} = \\frac{6}{3} = 2$$\n",
        "$$\\overline{Y} = \\frac{3+5+7}{3} = \\frac{15}{3} = 5$$\n",
        "\n",
        "**Step 2: Calculate slope**\n",
        "\n",
        "Numerator:\n",
        "$$\\sum(x_i - \\overline{X})(y_i - \\overline{Y}) = (1-2)(3-5) + (2-2)(5-5) + (3-2)(7-5)$$\n",
        "$$= (-1)(-2) + (0)(0) + (1)(2) = 2 + 0 + 2 = 4$$\n",
        "\n",
        "Denominator:\n",
        "$$\\sum(x_i - \\overline{X})^2 = (-1)^2 + (0)^2 + (1)^2 = 1 + 0 + 1 = 2$$\n",
        "\n",
        "$$m = \\frac{4}{2} = 2$$\n",
        "\n",
        "**Step 3: Calculate intercept**\n",
        "$$b = \\overline{Y} - m\\overline{X} = 5 - 2(2) = 5 - 4 = 1$$\n",
        "\n",
        "**Final Equation:** $y = 2x + 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28. For the data in Q27, construct the design matrix $\\Phi$ and verify the solution using Normal Equations.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Design Matrix:**\n",
        "$$\\Phi = \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix}$$\n",
        "\n",
        "**Target Vector:**\n",
        "$$y = \\begin{bmatrix} 3 \\\\ 5 \\\\ 7 \\end{bmatrix}$$\n",
        "\n",
        "**Step 1:** Calculate $\\Phi^T\\Phi$\n",
        "$$\\Phi^T\\Phi = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix} = \\begin{bmatrix} 14 & 6 \\\\ 6 & 3 \\end{bmatrix}$$\n",
        "\n",
        "**Step 2:** Calculate $\\Phi^Ty$\n",
        "$$\\Phi^Ty = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 5 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix} 34 \\\\ 15 \\end{bmatrix}$$\n",
        "\n",
        "**Step 3:** Calculate $(\\Phi^T\\Phi)^{-1}$\n",
        "$$\\det = 14(3) - 6(6) = 42 - 36 = 6$$\n",
        "$$(\\Phi^T\\Phi)^{-1} = \\frac{1}{6}\\begin{bmatrix} 3 & -6 \\\\ -6 & 14 \\end{bmatrix} = \\begin{bmatrix} 0.5 & -1 \\\\ -1 & 2.33 \\end{bmatrix}$$\n",
        "\n",
        "**Step 4:** Calculate $\\theta^*$\n",
        "$$\\theta^* = \\begin{bmatrix} 0.5 & -1 \\\\ -1 & 2.33 \\end{bmatrix} \\begin{bmatrix} 34 \\\\ 15 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$$\n",
        "\n",
        "**Result:** $\\theta_1 = 2$ (slope), $\\theta_2 = 1$ (intercept) ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q29. Calculate the cost $J(\\theta)$ for the model $y = 2x + 1$ on the data from Q27.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Data and Predictions:**\n",
        "\n",
        "| $x_i$ | $y_i$ (actual) | $\\hat{y}_i = 2x_i + 1$ | Error | Squared Error |\n",
        "|-------|----------------|------------------------|-------|---------------|\n",
        "| 1 | 3 | $2(1) + 1 = 3$ | 0 | 0 |\n",
        "| 2 | 5 | $2(2) + 1 = 5$ | 0 | 0 |\n",
        "| 3 | 7 | $2(3) + 1 = 7$ | 0 | 0 |\n",
        "\n",
        "**Cost Function:**\n",
        "$$J(\\theta) = \\sum_{i=1}^{3} (\\hat{y}_i - y_i)^2 = 0 + 0 + 0 = 0$$\n",
        "\n",
        "**Interpretation:**\n",
        "The cost is 0 because all points lie exactly on the line. This is a perfect fit (which is rare in real data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q30. Given data points (1,2), (2,3), (3,5), (4,4), calculate the regression line.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Step 1: Calculate means**\n",
        "$$\\overline{X} = \\frac{1+2+3+4}{4} = \\frac{10}{4} = 2.5$$\n",
        "$$\\overline{Y} = \\frac{2+3+5+4}{4} = \\frac{14}{4} = 3.5$$\n",
        "\n",
        "**Step 2: Calculate deviations**\n",
        "\n",
        "| $x_i$ | $y_i$ | $x_i - \\overline{X}$ | $y_i - \\overline{Y}$ | Product | $(x_i-\\overline{X})^2$ |\n",
        "|-------|-------|----------------------|----------------------|---------|------------------------|\n",
        "| 1 | 2 | -1.5 | -1.5 | 2.25 | 2.25 |\n",
        "| 2 | 3 | -0.5 | -0.5 | 0.25 | 0.25 |\n",
        "| 3 | 5 | 0.5 | 1.5 | 0.75 | 0.25 |\n",
        "| 4 | 4 | 1.5 | 0.5 | 0.75 | 2.25 |\n",
        "| **Sum** | | | | **4.0** | **5.0** |\n",
        "\n",
        "**Step 3: Calculate slope**\n",
        "$$m = \\frac{4.0}{5.0} = 0.8$$\n",
        "\n",
        "**Step 4: Calculate intercept**\n",
        "$$b = 3.5 - 0.8(2.5) = 3.5 - 2.0 = 1.5$$\n",
        "\n",
        "**Final Equation:** $y = 0.8x + 1.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Conceptual Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q31. Why is linear regression called \"linear\"? Is it limited to straight lines?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Why \"Linear\":**\n",
        "Linear regression is called linear because the prediction is a **linear combination of the parameters** $\\theta$.\n",
        "\n",
        "$$\\hat{y} = \\theta_1 \\phi_1(x) + \\theta_2 \\phi_2(x) + ... + \\theta_k \\phi_k(x)$$\n",
        "\n",
        "The model is linear **in $\\theta$**, not necessarily in $x$.\n",
        "\n",
        "**Not Limited to Straight Lines:**\n",
        "\n",
        "We can use non-linear feature mappings:\n",
        "\n",
        "| Feature Mapping | Resulting Model |\n",
        "|-----------------|------------------|\n",
        "| $\\phi(x) = [x, 1]^T$ | $\\hat{y} = \\theta_1 x + \\theta_2$ (line) |\n",
        "| $\\phi(x) = [x^2, x, 1]^T$ | $\\hat{y} = \\theta_1 x^2 + \\theta_2 x + \\theta_3$ (parabola) |\n",
        "| $\\phi(x) = [\\sin(x), \\cos(x), 1]^T$ | $\\hat{y} = \\theta_1 \\sin(x) + \\theta_2 \\cos(x) + \\theta_3$ |\n",
        "\n",
        "All these are still \"linear regression\" because they're linear in $\\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q32. Compare the analytical solution (Normal Equations) vs iterative solution (Gradient Descent).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect | Normal Equations | Gradient Descent |\n",
        "|--------|------------------|------------------|\n",
        "| **Formula** | $\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$ | $\\theta := \\theta - \\alpha \\nabla J$ |\n",
        "| **Type** | Closed-form/Analytical | Iterative |\n",
        "| **Iterations** | One-shot | Multiple iterations |\n",
        "| **Hyperparameters** | None | Learning rate $\\alpha$ |\n",
        "| **Time Complexity** | $O(k^3)$ for inverse | $O(mk)$ per iteration |\n",
        "| **Large $k$** | Slow (matrix inverse) | Fast |\n",
        "| **Large $m$** | Need all data in memory | Can use mini-batches |\n",
        "| **When to use** | Small to medium $k$ | Large $k$ or online learning |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q33. What happens if $\\Phi^T\\Phi$ is not invertible? What causes this?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**What Happens:**\n",
        "- The Normal Equations cannot be used directly\n",
        "- No unique solution exists\n",
        "- Infinitely many solutions may exist\n",
        "\n",
        "**Causes:**\n",
        "\n",
        "1. **Redundant features:** Two features are linearly dependent\n",
        "   - Example: \"height in cm\" and \"height in inches\"\n",
        "\n",
        "2. **More features than samples:** $k > m$\n",
        "   - Not enough data to determine all parameters\n",
        "\n",
        "3. **Duplicate features:** Same feature included twice\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "1. **Remove redundant features**\n",
        "2. **Use pseudo-inverse:** $\\theta = \\Phi^+ y$\n",
        "3. **Add regularization (Ridge):** $\\theta = (\\Phi^T\\Phi + \\lambda I)^{-1}\\Phi^Ty$\n",
        "4. **Use gradient descent instead**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q34. Explain the geometric interpretation of the least squares solution.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Geometric View:**\n",
        "\n",
        "1. **Column space of $\\Phi$:** The set of all possible predictions $\\Phi\\theta$\n",
        "\n",
        "2. **Target $y$:** May not lie in the column space of $\\Phi$\n",
        "\n",
        "3. **Optimal $\\hat{y} = \\Phi\\theta^*$:** The projection of $y$ onto the column space of $\\Phi$\n",
        "\n",
        "4. **Residual $(y - \\Phi\\theta^*)$:** Perpendicular to the column space\n",
        "\n",
        "**Key Insight:**\n",
        "The least squares solution finds the point in the \"prediction space\" that is closest to the actual target.\n",
        "\n",
        "**Why Normal Equations Work:**\n",
        "The condition $\\Phi^T(\\Phi\\theta - y) = 0$ means the residual is orthogonal to all columns of $\\Phi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q35. What is the role of the intercept term (bias) in linear regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Role of Intercept:**\n",
        "\n",
        "1. **Shifts the line:** Allows the line to not pass through origin\n",
        "\n",
        "2. **Baseline prediction:** The predicted value when all features are zero\n",
        "\n",
        "3. **Flexibility:** Without intercept, model is too constrained\n",
        "\n",
        "**Example:**\n",
        "- Predicting house price based on size\n",
        "- Without intercept: A 0 sq.ft house has price = \\$0\n",
        "- With intercept: A 0 sq.ft house has price = \\$base\\_price (land value, etc.)\n",
        "\n",
        "**How to Include:**\n",
        "Add a column of 1s to the feature matrix:\n",
        "$$\\phi(x) = [x_1, x_2, ..., x_n, 1]^T$$\n",
        "\n",
        "The parameter corresponding to the 1 becomes the intercept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q36. List the assumptions of linear regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Key Assumptions:**\n",
        "\n",
        "1. **Linearity:** Relationship between X and Y is linear (in parameters)\n",
        "\n",
        "2. **Independence:** Observations are independent of each other\n",
        "\n",
        "3. **Homoscedasticity:** Variance of errors is constant across all levels of X\n",
        "\n",
        "4. **No multicollinearity:** Features are not highly correlated with each other\n",
        "\n",
        "5. **Normality of errors:** Errors follow a normal distribution (for inference)\n",
        "\n",
        "6. **No autocorrelation:** Errors are not correlated with each other\n",
        "\n",
        "**When Assumptions Violated:**\n",
        "- Model may give biased or unreliable estimates\n",
        "- Need to use different techniques (regularization, transformations, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Summary Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q37. Summarize the complete pipeline for linear regression from data to prediction.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Complete Pipeline:**\n",
        "\n",
        "**Step 1: Data Collection**\n",
        "- Gather input-output pairs: $(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)$\n",
        "\n",
        "**Step 2: Feature Engineering**\n",
        "- Create feature mapping: $\\phi(x_i) = [x_i, 1]^T$ (or more complex)\n",
        "\n",
        "**Step 3: Construct Design Matrix**\n",
        "- Build $\\Phi \\in \\mathbb{R}^{m \\times k}$ from all feature vectors\n",
        "\n",
        "**Step 4: Solve Normal Equations**\n",
        "$$\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
        "\n",
        "**Step 5: Make Predictions**\n",
        "- For new input $x_{new}$:\n",
        "$$\\hat{y}_{new} = \\theta^{*T}\\phi(x_{new})$$\n",
        "\n",
        "**Step 6: Evaluate**\n",
        "- Calculate error on test data\n",
        "- Check if model is good enough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q38. What makes linear regression special compared to other ML algorithms?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Special Properties of Linear Regression:**\n",
        "\n",
        "1. **Analytical Solution:** One of the few ML problems with a closed-form solution\n",
        "   $$\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
        "\n",
        "2. **Interpretability:** Coefficients directly tell us feature importance\n",
        "\n",
        "3. **Simplicity:** Easy to understand and implement\n",
        "\n",
        "4. **Speed:** Fast to train (just matrix operations)\n",
        "\n",
        "5. **Foundation:** Basis for many advanced techniques\n",
        "\n",
        "6. **No hyperparameters:** (in basic form) No tuning required\n",
        "\n",
        "7. **Convex optimization:** Always finds global minimum\n",
        "\n",
        "**Limitations:**\n",
        "- Cannot model complex non-linear relationships directly\n",
        "- Sensitive to outliers\n",
        "- Assumes linear relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary of Important Formulas\n",
        "\n",
        "| Concept | Formula |\n",
        "|---------|--------|\n",
        "| Linear Model | $\\hat{y} = \\theta_1 x + \\theta_2$ |\n",
        "| Slope | $m = \\frac{\\sum(x_i - \\overline{X})(y_i - \\overline{Y})}{\\sum(x_i - \\overline{X})^2}$ |\n",
        "| Intercept | $b = \\overline{Y} - m\\overline{X}$ |\n",
        "| Prediction (vector) | $\\hat{y}_i = \\theta^T\\phi(x_i)$ |\n",
        "| Squared Loss | $l(\\hat{y}_i, y_i) = (\\hat{y}_i - y_i)^2$ |\n",
        "| Cost Function | $J(\\theta) = \\sum_{i=1}^{m}(\\theta^T\\phi(x_i) - y_i)^2$ |\n",
        "| Matrix Cost | $J(\\theta) = \\|\\Phi\\theta - y\\|_2^2$ |\n",
        "| Expanded Cost | $J(\\theta) = \\theta^T\\Phi^T\\Phi\\theta - 2y^T\\Phi\\theta + y^Ty$ |\n",
        "| Gradient | $\\nabla_\\theta J = 2\\Phi^T\\Phi\\theta - 2\\Phi^Ty$ |\n",
        "| Normal Equations | $\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$ |\n",
        "\n",
        "---"
      ]
    }
  ]
}