{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 11: Overfitting, Regularization & Multi-class Classification, Extended Classifier Evaluation\n",
        "## Possible Subjective Exam Questions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Overfitting and Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. What is overfitting and underfitting? Explain with examples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Overfitting:**\n",
        "- Model learns the training data too well, including noise and random patterns\n",
        "- Very low training error but high test error\n",
        "- Model is too complex for the data\n",
        "- Memorizes instead of generalizes\n",
        "\n",
        "**Example:** A model that perfectly fits all training points by drawing a very wiggly curve through them, but fails on new data.\n",
        "\n",
        "**Underfitting:**\n",
        "- Model is too simple to capture the underlying pattern\n",
        "- High training error and high test error\n",
        "- Model cannot learn the relationship in data\n",
        "\n",
        "**Example:** Using a straight line to fit data that clearly has a curved pattern.\n",
        "\n",
        "| Aspect | Underfitting | Good Fit | Overfitting |\n",
        "|--------|--------------|----------|-------------|\n",
        "| Model Complexity | Too Low | Just Right | Too High |\n",
        "| Training Error | High | Low | Very Low |\n",
        "| Test Error | High | Low | High |\n",
        "| Bias | High | Low | Low |\n",
        "| Variance | Low | Low | High |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. How can you identify if a model is overfitting or underfitting?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Signs of Overfitting:**\n",
        "1. Training accuracy is very high (close to 100%)\n",
        "2. Test/validation accuracy is much lower than training accuracy\n",
        "3. Large gap between training and test error\n",
        "4. Model performs poorly on new, unseen data\n",
        "\n",
        "**Signs of Underfitting:**\n",
        "1. Training accuracy itself is low\n",
        "2. Both training and test accuracy are poor\n",
        "3. Model fails to capture obvious patterns\n",
        "4. Adding more training data doesn't help much\n",
        "\n",
        "**How to Detect:**\n",
        "- Plot learning curves (training vs validation error)\n",
        "- Use cross-validation\n",
        "- Compare performance on training vs test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. What is regularization? Why is it used?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Regularization:**\n",
        "A technique to prevent overfitting by adding a penalty term to the cost function that discourages complex models.\n",
        "\n",
        "**Why It's Used:**\n",
        "\n",
        "1. **Prevents overfitting:** Keeps model parameters small\n",
        "2. **Improves generalization:** Model performs better on unseen data\n",
        "3. **Reduces variance:** Model becomes more stable\n",
        "4. **Handles multicollinearity:** Helps when features are correlated\n",
        "\n",
        "**Common Types:**\n",
        "\n",
        "| Type | Penalty Term | Effect |\n",
        "|------|--------------|--------|\n",
        "| L2 (Ridge) | $\\lambda \\sum_{j} \\theta_j^2$ | Shrinks weights, keeps all features |\n",
        "| L1 (Lasso) | $\\lambda \\sum_{j} |\\theta_j|$ | Can make some weights exactly zero |\n",
        "\n",
        "**Regularized Cost Function:**\n",
        "$$J_{regularized}(\\theta) = J(\\theta) + \\lambda \\cdot R(\\theta)$$\n",
        "\n",
        "Where $\\lambda$ controls the regularization strength."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. How does the regularization parameter λ affect the model?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Effect of λ (Lambda):**\n",
        "\n",
        "| λ Value | Effect on Model | Risk |\n",
        "|---------|-----------------|------|\n",
        "| λ = 0 | No regularization | Overfitting |\n",
        "| λ too small | Weak regularization | Still may overfit |\n",
        "| λ optimal | Good balance | Best generalization |\n",
        "| λ too large | Strong regularization | Underfitting |\n",
        "| λ → ∞ | All weights → 0 | Severe underfitting |\n",
        "\n",
        "**Tradeoff:**\n",
        "- **Small λ:** Model can fit training data well but may overfit\n",
        "- **Large λ:** Model becomes too simple, may underfit\n",
        "\n",
        "**How to Choose λ:**\n",
        "1. Cross-validation\n",
        "2. Grid search\n",
        "3. Validation set performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Multi-class Classification Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. What is multi-class classification? How is it different from binary classification?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Multi-class Classification:**\n",
        "Classification problem where there are more than two possible classes.\n",
        "\n",
        "**Formal Definition:**\n",
        "- Given training data: $\\{(x_i, y_i) : 1 \\leq i \\leq n\\}$\n",
        "- Input: $x_i \\in \\mathbb{R}^d$\n",
        "- Output: $y_i \\in \\{1, 2, ..., K\\}$ where $K > 2$\n",
        "- Data is i.i.d. (Independent and Identically Distributed) from distribution $D$\n",
        "- Goal: Find $f(x): \\mathbb{R}^d \\rightarrow \\{1, 2, ..., K\\}$ that outputs correct labels\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "| Aspect | Binary Classification | Multi-class Classification |\n",
        "|--------|----------------------|---------------------------|\n",
        "| Classes | 2 (e.g., 0 and 1) | K > 2 (e.g., 1, 2, ..., K) |\n",
        "| Example | Spam/Not Spam | Cat/Dog/Bird/Fish |\n",
        "| Output | Single probability | K probabilities |\n",
        "| Decision | One threshold | Compare K scores |\n",
        "\n",
        "**Examples:**\n",
        "- Binary: Indoor vs Outdoor image\n",
        "- Multi-class: ImageNet with 1000 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. What does i.i.d. mean in the context of training data?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**i.i.d. = Independent and Identically Distributed**\n",
        "\n",
        "**Independent:**\n",
        "- Each data sample is collected independently\n",
        "- One sample doesn't affect another\n",
        "- No correlation between different samples\n",
        "\n",
        "**Identically Distributed:**\n",
        "- All samples come from the same probability distribution $D$\n",
        "- The distribution doesn't change over time\n",
        "- Training and test data follow same distribution\n",
        "\n",
        "**Why It Matters:**\n",
        "1. Most ML theory assumes i.i.d. data\n",
        "2. Allows us to use statistical guarantees\n",
        "3. Ensures training data represents the test data\n",
        "\n",
        "**When i.i.d. is Violated:**\n",
        "- Time series data (samples are dependent)\n",
        "- Distribution shift (training ≠ test distribution)\n",
        "- Clustered/grouped data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Approach 1 - Reduce to Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. Why is reducing multi-class classification to linear regression a bad idea?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**The Approach:**\n",
        "- Ignore that $y \\in \\{1, 2, ..., K\\}$ is categorical\n",
        "- Treat class labels as numerical values\n",
        "- Find $f_w(x) = w^T x$ that minimizes:\n",
        "$$L(f_w) = \\frac{1}{n} \\sum_{i=1}^{n} (w^T x_i - y_i)^2$$\n",
        "\n",
        "**Why It's Bad:**\n",
        "\n",
        "1. **Arbitrary ordering:** Class labels have no natural order\n",
        "   - Why is class 2 \"between\" class 1 and class 3?\n",
        "   - Distance between classes is meaningless\n",
        "\n",
        "2. **Wrong loss function:** MSE penalizes based on numerical distance\n",
        "   - Predicting class 3 for class 1 penalized more than predicting class 2\n",
        "   - But both are equally wrong!\n",
        "\n",
        "3. **Unbounded output:** Regression output can be any real number\n",
        "   - What if model predicts 2.7? Which class?\n",
        "\n",
        "4. **Even bad for binary:** Sensitive to outliers, decision boundary shifts\n",
        "\n",
        "**Conclusion:** This approach is a BAD IDEA even for binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. Illustrate with an example why regression fails for classification.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Example: Binary Classification (Stroke vs Drug Overdose)**\n",
        "\n",
        "Suppose we have patients with features and labels:\n",
        "- Class 0: Stroke\n",
        "- Class 1: Drug Overdose\n",
        "\n",
        "**Problem 1: Outliers Shift the Line**\n",
        "\n",
        "If we add a patient with extreme features who has drug overdose (class 1):\n",
        "- The regression line shifts to accommodate this outlier\n",
        "- Other points that were correctly classified may now be wrong\n",
        "- Threshold of 0.5 no longer works well\n",
        "\n",
        "**Problem 2: Predictions Outside [0,1]**\n",
        "\n",
        "| Patient | Regression Prediction | Interpretation |\n",
        "|---------|----------------------|----------------|\n",
        "| A | 0.3 | Class 0 ✓ |\n",
        "| B | 0.7 | Class 1 ✓ |\n",
        "| C | 1.5 | What class? |\n",
        "| D | -0.2 | What class? |\n",
        "\n",
        "**The Bishop's Figure Shows:**\n",
        "How adding a few points on the right can shift the decision boundary incorrectly, misclassifying many points that were correct before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Approach 2 - One-versus-the-Rest (OvR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. Explain the One-versus-the-Rest (OvR) approach for multi-class classification.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**One-versus-the-Rest (OvR) / One-versus-All (OvA):**\n",
        "\n",
        "**Concept:**\n",
        "Train $K-1$ binary classifiers, each separating one class from all others.\n",
        "\n",
        "**Classifiers:**\n",
        "- $f_1$: Classifies class 1 vs $\\{2, 3, ..., K\\}$\n",
        "- $f_2$: Classifies class 2 vs $\\{1, 3, ..., K\\}$\n",
        "- ...\n",
        "- $f_{K-1}$: Classifies class $K-1$ vs $\\{1, 2, ..., K-2, K\\}$\n",
        "\n",
        "**Classification Rule:**\n",
        "- Points classified as class $i$ by $f_i$ belong to class $i$\n",
        "- Points not classified by any $f_1, ..., f_{K-1}$ are assigned to class $K$\n",
        "\n",
        "**Number of Classifiers:** $K - 1$\n",
        "\n",
        "**Problem: Ambiguous Regions**\n",
        "- Some points may be classified to more than one class\n",
        "- Some points may not be classified by any classifier\n",
        "- Decision boundaries may overlap or leave gaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. What is the problem of ambiguous regions in One-versus-the-Rest?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**The Problem:**\n",
        "\n",
        "When using multiple binary classifiers, their decision boundaries can:\n",
        "\n",
        "1. **Overlap:** A point is claimed by multiple classifiers\n",
        "   - $f_1$ says it's class 1\n",
        "   - $f_2$ says it's class 2\n",
        "   - Which one is correct?\n",
        "\n",
        "2. **Leave gaps:** A point is not claimed by any classifier\n",
        "   - None of $f_1, f_2, ..., f_{K-1}$ claim it\n",
        "   - Just assigning to class $K$ may be wrong\n",
        "\n",
        "**Visual Example:**\n",
        "Imagine 3 classes in 2D space:\n",
        "- Class 1 region overlaps with Class 2 region\n",
        "- Some areas are \"no man's land\"\n",
        "\n",
        "**Solutions:**\n",
        "1. Use confidence scores and pick highest\n",
        "2. Use probability estimates\n",
        "3. Use different approaches (discriminant functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Approach 3 - One-versus-One (OvO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11. Explain the One-versus-One (OvO) approach for multi-class classification.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**One-versus-One (OvO):**\n",
        "\n",
        "**Concept:**\n",
        "Train a binary classifier for every pair of classes.\n",
        "\n",
        "**Classifiers:**\n",
        "- $f_{(1,2)}$: Classifies class 1 vs class 2\n",
        "- $f_{(1,3)}$: Classifies class 1 vs class 3\n",
        "- ...\n",
        "- $f_{(K-1,K)}$: Classifies class $K-1$ vs class $K$\n",
        "\n",
        "**Number of Classifiers:**\n",
        "$$\\frac{K(K-1)}{2}$$\n",
        "\n",
        "**Classification Rule (Voting):**\n",
        "- Each classifier votes for one class\n",
        "- Final prediction = class with most votes\n",
        "\n",
        "**Example with K=4:**\n",
        "- Number of classifiers = $\\frac{4 \\times 3}{2} = 6$\n",
        "- Classifiers: $f_{(1,2)}, f_{(1,3)}, f_{(1,4)}, f_{(2,3)}, f_{(2,4)}, f_{(3,4)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12. Compare One-versus-the-Rest and One-versus-One approaches.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect | One-vs-Rest (OvR) | One-vs-One (OvO) |\n",
        "|--------|-------------------|------------------|\n",
        "| **Number of Classifiers** | $K - 1$ | $\\frac{K(K-1)}{2}$ |\n",
        "| **For K=10** | 9 classifiers | 45 classifiers |\n",
        "| **For K=1000** | 999 classifiers | 499,500 classifiers |\n",
        "| **Training Data per Classifier** | All data | Only 2 classes |\n",
        "| **Training Time** | Less | More (many classifiers) |\n",
        "| **Ambiguous Regions** | Yes | Yes |\n",
        "| **Imbalanced Data** | Severe (1 vs many) | Less severe (1 vs 1) |\n",
        "\n",
        "**OvO Advantage:**\n",
        "- Each classifier only sees two classes\n",
        "- Smaller, more balanced training sets\n",
        "\n",
        "**OvO Disadvantage:**\n",
        "- Computationally expensive for large K\n",
        "- Think of ImageNet with K=1000: Need 499,500 classifiers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13. Calculate the number of classifiers needed for OvR and OvO when K=5.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:** K = 5 classes\n",
        "\n",
        "**One-versus-Rest (OvR):**\n",
        "$$\\text{Number of classifiers} = K - 1 = 5 - 1 = 4$$\n",
        "\n",
        "Classifiers:\n",
        "- $f_1$: Class 1 vs {2,3,4,5}\n",
        "- $f_2$: Class 2 vs {1,3,4,5}\n",
        "- $f_3$: Class 3 vs {1,2,4,5}\n",
        "- $f_4$: Class 4 vs {1,2,3,5}\n",
        "- (Class 5 gets remaining points)\n",
        "\n",
        "**One-versus-One (OvO):**\n",
        "$$\\text{Number of classifiers} = \\frac{K(K-1)}{2} = \\frac{5 \\times 4}{2} = 10$$\n",
        "\n",
        "Classifiers:\n",
        "- $f_{(1,2)}, f_{(1,3)}, f_{(1,4)}, f_{(1,5)}$\n",
        "- $f_{(2,3)}, f_{(2,4)}, f_{(2,5)}$\n",
        "- $f_{(3,4)}, f_{(3,5)}$\n",
        "- $f_{(4,5)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Approach 4 - Discriminant Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14. What are discriminant functions? How do they solve the ambiguity problem?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Discriminant Functions:**\n",
        "\n",
        "**Concept:**\n",
        "Learn K scoring functions $s_1, s_2, ..., s_K$, one for each class.\n",
        "\n",
        "**Classification Rule:**\n",
        "$$y = \\arg\\max_i s_i(x)$$\n",
        "\n",
        "Assign $x$ to the class with the highest score.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. **Computationally cheap:** Only K functions, not $\\frac{K(K-1)}{2}$\n",
        "\n",
        "2. **No ambiguous regions:** \n",
        "   - Every point gets exactly one class\n",
        "   - argmax always picks one winner\n",
        "   - No overlaps, no gaps\n",
        "\n",
        "3. **Direct approach:** Directly models class membership\n",
        "\n",
        "**Example:**\n",
        "For K=3 and input $x$:\n",
        "- $s_1(x) = 2.5$\n",
        "- $s_2(x) = 3.1$\n",
        "- $s_3(x) = 1.8$\n",
        "\n",
        "Classification: $y = \\arg\\max(2.5, 3.1, 1.8) = 2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15. Explain Linear Discriminant Functions and their properties.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Linear Discriminant Functions:**\n",
        "\n",
        "**Definition:**\n",
        "$$s_i(x) = w_i^T x$$\n",
        "\n",
        "Where $w_i \\in \\mathbb{R}^d$ is the weight vector for class $i$.\n",
        "\n",
        "**Classification:**\n",
        "$$y = \\arg\\max_i (w_i^T x)$$\n",
        "\n",
        "**Key Property: Convex Regions**\n",
        "\n",
        "Each class gets a convex region in feature space.\n",
        "\n",
        "**Why Convex?**\n",
        "- Decision boundary between class $i$ and $j$ is:\n",
        "  $$w_i^T x = w_j^T x$$\n",
        "  $$(w_i - w_j)^T x = 0$$\n",
        "- This is a hyperplane (linear boundary)\n",
        "- Intersection of half-spaces = convex region\n",
        "\n",
        "**Advantages:**\n",
        "1. Simple and interpretable\n",
        "2. Fast to compute\n",
        "3. No ambiguous regions\n",
        "\n",
        "**Limitation:**\n",
        "Can only create linear boundaries between classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q16. What are conditional distributions as discriminant functions?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Conditional Distribution Approach:**\n",
        "\n",
        "**Definition:**\n",
        "Use the posterior probability as the scoring function:\n",
        "$$s_i(x) = P(y = i | x)$$\n",
        "\n",
        "**Parametrized Form:**\n",
        "$$s_i(x) = P_{w_i}(y = i | x)$$\n",
        "\n",
        "Where $w_i$ are learnable parameters.\n",
        "\n",
        "**Classification:**\n",
        "$$y = \\arg\\max_i P(y = i | x)$$\n",
        "\n",
        "**Properties:**\n",
        "\n",
        "1. **Probabilistic interpretation:** Outputs are probabilities\n",
        "\n",
        "2. **Sum to 1:** $\\sum_{i=1}^{K} P(y = i | x) = 1$\n",
        "\n",
        "3. **Confidence measure:** Know how confident the prediction is\n",
        "\n",
        "**Examples:**\n",
        "- Softmax regression (multinomial logistic regression)\n",
        "- Neural networks with softmax output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17. Compare all four approaches to multi-class classification.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Approach | # Models | Ambiguity | Complexity | Recommended? |\n",
        "|----------|----------|-----------|------------|-------------|\n",
        "| **Regression** | 1 | Yes | Low | NO ❌ |\n",
        "| **One-vs-Rest** | K-1 | Yes | Medium | Sometimes |\n",
        "| **One-vs-One** | K(K-1)/2 | Yes | High | Sometimes |\n",
        "| **Discriminant** | K | No | Low | YES ✓ |\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "1. **Regression:** Bad idea - don't use for classification\n",
        "\n",
        "2. **One-vs-Rest:** \n",
        "   - Good for some algorithms (e.g., SVM)\n",
        "   - Imbalanced training data issue\n",
        "\n",
        "3. **One-vs-One:**\n",
        "   - Better balanced training\n",
        "   - Too expensive for large K\n",
        "\n",
        "4. **Discriminant Functions:**\n",
        "   - Best overall approach\n",
        "   - Used in softmax regression, neural networks\n",
        "   - No ambiguity, computationally efficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Classifier Evaluation Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18. Define True Positive, False Positive, True Negative, and False Negative with examples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Context: Disease Testing**\n",
        "\n",
        "| Term | Definition | Example |\n",
        "|------|------------|----------|\n",
        "| **True Positive (TP)** | Test says positive, actually positive | Test says disease, patient has disease |\n",
        "| **False Positive (FP)** | Test says positive, actually negative | Test says disease, patient is healthy |\n",
        "| **True Negative (TN)** | Test says negative, actually negative | Test says no disease, patient is healthy |\n",
        "| **False Negative (FN)** | Test says negative, actually positive | Test says no disease, patient has disease |\n",
        "\n",
        "**Confusion Matrix:**\n",
        "\n",
        "```\n",
        "                    Predicted\n",
        "                 Positive  Negative\n",
        "Actual  Positive    TP        FN\n",
        "        Negative    FP        TN\n",
        "```\n",
        "\n",
        "**Memory Trick:**\n",
        "- First word (True/False): Was the prediction correct?\n",
        "- Second word (Positive/Negative): What did the model predict?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19. Define and derive the formulas for TPR, TNR, FPR, and FNR.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**True Positive Rate (TPR) / Sensitivity / Recall:**\n",
        "$$\\text{TPR} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Probability of positive test result among those who actually have the disease.\n",
        "\n",
        "**True Negative Rate (TNR) / Specificity:**\n",
        "$$\\text{TNR} = \\frac{TN}{TN + FP}$$\n",
        "\n",
        "Probability of negative test result among those who don't have the disease.\n",
        "\n",
        "**False Positive Rate (FPR):**\n",
        "$$\\text{FPR} = \\frac{FP}{TN + FP} = 1 - \\text{TNR}$$\n",
        "\n",
        "Percent of 0s classified as 1.\n",
        "\n",
        "**False Negative Rate (FNR):**\n",
        "$$\\text{FNR} = \\frac{FN}{TP + FN} = 1 - \\text{TPR}$$\n",
        "\n",
        "Percent of 1s classified as 0.\n",
        "\n",
        "**Relationships:**\n",
        "- $\\text{FPR} + \\text{TNR} = 1$\n",
        "- $\\text{FNR} + \\text{TPR} = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Tradeoffs and Threshold Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20. Why do different applications require different tradeoffs between FPR and FNR?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Key Insight:** Mistakes have different costs in different applications!\n",
        "\n",
        "**Disease Screening:**\n",
        "- Need: LOW False Negative Rate\n",
        "- Why: Missing a disease (FN) is very dangerous\n",
        "- Acceptable: Higher FP (extra tests for healthy people)\n",
        "\n",
        "**Spam Filtering:**\n",
        "- Need: LOW False Positive Rate\n",
        "- Why: Blocking important email (FP) is costly\n",
        "- Acceptable: Higher FN (some spam in inbox)\n",
        "\n",
        "**Example from Slides:**\n",
        "\n",
        "| Model | FPR | FNR | Best For |\n",
        "|-------|-----|-----|----------|\n",
        "| Model 1 | 41% | 3% | When FN is costly |\n",
        "| Model 2 | 5% | 25% | When FP is costly |\n",
        "\n",
        "**Key Point:** These could be the SAME model with different thresholds!\n",
        "\n",
        "**Conservative vs Aggressive:**\n",
        "Same application may need multiple tradeoffs for different users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21. How does changing the threshold affect classification in logistic regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Logistic Regression Output:**\n",
        "- Produces a score between 0 and 1 (probability estimate)\n",
        "- Use threshold to convert to classification\n",
        "\n",
        "**Effect of Threshold:**\n",
        "\n",
        "| Threshold | Effect | FPR | FNR |\n",
        "|-----------|--------|-----|-----|\n",
        "| Low (e.g., 0.3) | More positive predictions | Higher | Lower |\n",
        "| High (e.g., 0.7) | Fewer positive predictions | Lower | Higher |\n",
        "\n",
        "**Example from Slides:**\n",
        "\n",
        "| Score | Y | Threshold=0.5 | Threshold=0.6 | Threshold=0.7 |\n",
        "|-------|---|---------------|---------------|---------------|\n",
        "| 0.25 | 0 | Pred=0 ✓ | Pred=0 ✓ | Pred=0 ✓ |\n",
        "| 0.45 | 0 | Pred=0 ✓ | Pred=0 ✓ | Pred=0 ✓ |\n",
        "| 0.55 | 1 | Pred=1 ✓ | Pred=0 ✗ | Pred=0 ✗ |\n",
        "| 0.67 | 0 | Pred=1 ✗ | Pred=1 ✗ | Pred=0 ✓ |\n",
        "| 0.82 | 1 | Pred=1 ✓ | Pred=1 ✓ | Pred=1 ✓ |\n",
        "| 0.95 | 1 | Pred=1 ✓ | Pred=1 ✓ | Pred=1 ✓ |\n",
        "\n",
        "| Threshold | FPR | FNR |\n",
        "|-----------|-----|-----|\n",
        "| 0.5 | 33% | 0% |\n",
        "| 0.6 | 33% | 33% |\n",
        "| 0.7 | 0% | 33% |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22. Calculate FPR and FNR for different thresholds given a dataset.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given Data:**\n",
        "\n",
        "| Score | Actual Y |\n",
        "|-------|----------|\n",
        "| 0.25 | 0 |\n",
        "| 0.45 | 0 |\n",
        "| 0.55 | 1 |\n",
        "| 0.67 | 0 |\n",
        "| 0.82 | 1 |\n",
        "| 0.95 | 1 |\n",
        "\n",
        "Total Positives (Y=1): 3\n",
        "Total Negatives (Y=0): 3\n",
        "\n",
        "**Threshold = 0.5:**\n",
        "- Predictions: 0,0,1,1,1,1\n",
        "- TP=3, FP=1, TN=2, FN=0\n",
        "- $\\text{FPR} = \\frac{1}{2+1} = \\frac{1}{3} = 33\\%$\n",
        "- $\\text{FNR} = \\frac{0}{3+0} = 0\\%$\n",
        "\n",
        "**Threshold = 0.7:**\n",
        "- Predictions: 0,0,0,0,1,1\n",
        "- TP=2, FP=0, TN=3, FN=1\n",
        "- $\\text{FPR} = \\frac{0}{3+0} = 0\\%$\n",
        "- $\\text{FNR} = \\frac{1}{2+1} = 33\\%$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23. What is an ROC curve? How is it constructed?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**ROC = Receiver Operating Characteristic**\n",
        "\n",
        "**Definition:**\n",
        "A plot showing the tradeoff between FPR and FNR (or TPR) at different thresholds.\n",
        "\n",
        "**Construction:**\n",
        "1. Sweep threshold from 0 to 1\n",
        "2. At each threshold, calculate FPR and FNR (or TPR)\n",
        "3. Plot FPR on x-axis, FNR on y-axis (or TPR on y-axis)\n",
        "\n",
        "**Extreme Points:**\n",
        "\n",
        "| Threshold | Effect | FPR | FNR |\n",
        "|-----------|--------|-----|-----|\n",
        "| 0 | All classified as 1 | 100% | 0% |\n",
        "| 1 | All classified as 0 | 0% | 100% |\n",
        "\n",
        "**Perfect Classifier:**\n",
        "- 0% of 1s called 0 (FNR = 0%)\n",
        "- 0% of 0s called 1 (FPR = 0%)\n",
        "- Point at origin (0,0)\n",
        "\n",
        "**Random Classifier:**\n",
        "- Diagonal line from (0,100%) to (100%,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24. How do you compare two models using ROC curves?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Comparison Method:**\n",
        "\n",
        "A model is better if its ROC curve is closer to the origin (lower left corner).\n",
        "\n",
        "**Case 1: One curve dominates**\n",
        "- If Model 1's curve is entirely below Model 2's curve\n",
        "- Model 1 is better at EVERY threshold\n",
        "- Model 1 has lower FNR for any given FPR\n",
        "- Model 1 has lower FPR for any given FNR\n",
        "\n",
        "**Case 2: Curves cross**\n",
        "- Neither model is universally better\n",
        "- Choice depends on application requirements\n",
        "- At some FPR targets, Model 1 is better\n",
        "- At other FPR targets, Model 2 is better\n",
        "\n",
        "**From Slides Example:**\n",
        "- Model 1 (AUC ~97) is better than Model 2 (AUC ~89.5)\n",
        "- Model 1's curve is closer to origin\n",
        "- Model 1 dominates at all operating points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25. What is Area Under the Curve (AUC)? What does it indicate?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**AUC = Area Under the ROC Curve**\n",
        "\n",
        "**Calculation:**\n",
        "Integrate the area under the ROC curve.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "| AUC Value | Interpretation |\n",
        "|-----------|----------------|\n",
        "| 1.0 | Perfect classifier |\n",
        "| 0.9 - 1.0 | Excellent |\n",
        "| 0.8 - 0.9 | Good |\n",
        "| 0.7 - 0.8 | Fair |\n",
        "| 0.5 | Random guessing |\n",
        "| < 0.5 | Worse than random (something wrong!) |\n",
        "\n",
        "**Properties:**\n",
        "\n",
        "1. **Threshold-independent:** Single number summarizing all thresholds\n",
        "\n",
        "2. **Probabilistic meaning:** Probability that model ranks a random positive higher than a random negative\n",
        "\n",
        "3. **Comparison:** Higher AUC = generally better tradeoffs available\n",
        "\n",
        "**Example from Slides:**\n",
        "- Model 1: AUC ≈ 0.97 (Excellent)\n",
        "- Model 2: AUC ≈ 0.895 (Good)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26. What does an AUC of 0.5 mean? What about AUC < 0.5?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**AUC = 0.5:**\n",
        "\n",
        "- Model is essentially randomly guessing\n",
        "- No better than flipping a coin\n",
        "- The model has no discriminative power\n",
        "- ROC curve follows the diagonal line\n",
        "\n",
        "**Possible Causes:**\n",
        "1. Features have no relationship with label\n",
        "2. Model hasn't learned anything useful\n",
        "3. Training failed\n",
        "\n",
        "**AUC < 0.5:**\n",
        "\n",
        "- Model is doing worse than random!\n",
        "- Something is wrong with the model or evaluation\n",
        "\n",
        "**Possible Causes:**\n",
        "1. Labels are flipped (0s and 1s swapped)\n",
        "2. Bug in evaluation code\n",
        "3. Model is confidently wrong\n",
        "\n",
        "**Fix:**\n",
        "- If AUC < 0.5, flip the predictions\n",
        "- Check for bugs in preprocessing or evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27. What is Precision? What is Recall? Write their formulas.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Precision:**\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "- Of all predicted positives, how many are actually positive?\n",
        "- Also called Positive Predictive Value (PPV)\n",
        "- High precision = few false alarms\n",
        "\n",
        "**Recall:**\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "- Of all actual positives, how many did we find?\n",
        "- Same as True Positive Rate (TPR) / Sensitivity\n",
        "- High recall = few missed positives\n",
        "\n",
        "**Example:**\n",
        "- TP=80, FP=20, FN=10\n",
        "- Precision = 80/(80+20) = 80%\n",
        "- Recall = 80/(80+10) = 89%\n",
        "\n",
        "**Tradeoff:**\n",
        "- Increasing recall often decreases precision\n",
        "- Finding more positives means more false positives too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28. Explain the Precision-Recall (PR) curve and its interpretation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**PR Curve:**\n",
        "Plot of Precision (y-axis) vs Recall (x-axis) at different thresholds.\n",
        "\n",
        "**Construction:**\n",
        "1. Start with threshold near 1\n",
        "2. Gradually lower threshold\n",
        "3. At each point, calculate Precision and Recall\n",
        "4. Plot the curve\n",
        "\n",
        "**Key Points on the Curve:**\n",
        "\n",
        "| Threshold | Recall | Precision | Interpretation |\n",
        "|-----------|--------|-----------|----------------|\n",
        "| Near 1 | Low | High | Only very confident positives |\n",
        "| Near 0 | High (~100%) | Low | Everything classified as positive |\n",
        "\n",
        "**Typical Shape:**\n",
        "- Starts at high precision, low recall (top-left)\n",
        "- Ends at low precision, high recall (bottom-right)\n",
        "- Good model: curve stays high for longer\n",
        "\n",
        "**From Slides:**\n",
        "- Near threshold 1: High precision, low recall\n",
        "- Near threshold 0: All 1s, precision around 25% (if 25% are positive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q29. When should you use PR curve instead of ROC curve?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Use PR Curve When:**\n",
        "\n",
        "1. **Imbalanced classes:**\n",
        "   - When positive class is rare (e.g., 1% fraud cases)\n",
        "   - ROC can be misleading with imbalanced data\n",
        "   - PR focuses on positive class performance\n",
        "\n",
        "2. **Positive class is more important:**\n",
        "   - When you care more about finding positives\n",
        "   - E.g., disease detection, fraud detection\n",
        "\n",
        "3. **True negatives are not interesting:**\n",
        "   - When correctly predicting negatives is easy\n",
        "   - PR doesn't use TN in its calculation\n",
        "\n",
        "**Use ROC Curve When:**\n",
        "\n",
        "1. Classes are roughly balanced\n",
        "2. Both types of errors are equally important\n",
        "3. You want threshold-independent comparison\n",
        "\n",
        "**Key Difference:**\n",
        "- ROC uses FPR (involves TN)\n",
        "- PR uses Precision (doesn't involve TN)\n",
        "- When TN is very large, FPR stays low even with many FP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Numerical Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q30. Given the confusion matrix below, calculate all relevant metrics.\n",
        "\n",
        "```\n",
        "                Predicted\n",
        "              Pos    Neg\n",
        "Actual  Pos   70     30\n",
        "        Neg   20     80\n",
        "```\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Extract Values:**\n",
        "- TP = 70, FN = 30\n",
        "- FP = 20, TN = 80\n",
        "- Total = 200\n",
        "\n",
        "**Accuracy:**\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{Total} = \\frac{70 + 80}{200} = \\frac{150}{200} = 75\\%$$\n",
        "\n",
        "**True Positive Rate (Sensitivity/Recall):**\n",
        "$$\\text{TPR} = \\frac{TP}{TP + FN} = \\frac{70}{70 + 30} = \\frac{70}{100} = 70\\%$$\n",
        "\n",
        "**True Negative Rate (Specificity):**\n",
        "$$\\text{TNR} = \\frac{TN}{TN + FP} = \\frac{80}{80 + 20} = \\frac{80}{100} = 80\\%$$\n",
        "\n",
        "**False Positive Rate:**\n",
        "$$\\text{FPR} = \\frac{FP}{TN + FP} = \\frac{20}{100} = 20\\%$$\n",
        "\n",
        "**False Negative Rate:**\n",
        "$$\\text{FNR} = \\frac{FN}{TP + FN} = \\frac{30}{100} = 30\\%$$\n",
        "\n",
        "**Precision:**\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{70}{70 + 20} = \\frac{70}{90} = 77.8\\%$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q31. For K=6 classes, calculate the number of classifiers needed for OvR and OvO.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:** K = 6 classes\n",
        "\n",
        "**One-versus-Rest (OvR):**\n",
        "$$\\text{Number of classifiers} = K - 1 = 6 - 1 = 5$$\n",
        "\n",
        "**One-versus-One (OvO):**\n",
        "$$\\text{Number of classifiers} = \\frac{K(K-1)}{2} = \\frac{6 \\times 5}{2} = \\frac{30}{2} = 15$$\n",
        "\n",
        "**Comparison:**\n",
        "- OvR needs 5 classifiers\n",
        "- OvO needs 15 classifiers\n",
        "- OvO needs 3× more classifiers than OvR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q32. Given discriminant scores for 3 classes, determine the predicted class.\n",
        "\n",
        "**Scores:** $s_1(x) = 2.3$, $s_2(x) = 1.8$, $s_3(x) = 2.7$\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given Scores:**\n",
        "- Class 1: $s_1(x) = 2.3$\n",
        "- Class 2: $s_2(x) = 1.8$\n",
        "- Class 3: $s_3(x) = 2.7$\n",
        "\n",
        "**Classification Rule:**\n",
        "$$y = \\arg\\max_i s_i(x)$$\n",
        "\n",
        "**Finding Maximum:**\n",
        "$$\\max(2.3, 1.8, 2.7) = 2.7$$\n",
        "\n",
        "This maximum corresponds to class 3.\n",
        "\n",
        "**Answer:** Predicted class = **3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 12: Conceptual Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q33. Why do linear discriminant functions lead to convex decision regions?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Linear Discriminant:**\n",
        "$$s_i(x) = w_i^T x$$\n",
        "\n",
        "**Classification:**\n",
        "$$y = \\arg\\max_i (w_i^T x)$$\n",
        "\n",
        "**Decision Boundary Between Class i and j:**\n",
        "\n",
        "Class $i$ is preferred when:\n",
        "$$w_i^T x > w_j^T x$$\n",
        "$$(w_i - w_j)^T x > 0$$\n",
        "\n",
        "This defines a half-space (one side of a hyperplane).\n",
        "\n",
        "**Why Convex:**\n",
        "\n",
        "The region for class $i$ is defined by:\n",
        "$$\\{x : w_i^T x > w_j^T x \\text{ for all } j \\neq i\\}$$\n",
        "\n",
        "This is an intersection of multiple half-spaces (one for each other class).\n",
        "\n",
        "**Mathematical Fact:**\n",
        "Intersection of half-spaces is always a convex set.\n",
        "\n",
        "**Implication:**\n",
        "- Each class occupies a convex region\n",
        "- Decision boundaries are linear (hyperplanes)\n",
        "- No \"islands\" or disconnected regions per class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q34. What is the relationship between the ROC curve and the confusion matrix?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Connection:**\n",
        "\n",
        "Each point on the ROC curve corresponds to a confusion matrix at a specific threshold.\n",
        "\n",
        "**ROC Curve Axes:**\n",
        "- X-axis: FPR = FP/(TN+FP)\n",
        "- Y-axis: TPR = TP/(TP+FN) (or FNR for alternative version)\n",
        "\n",
        "**As Threshold Changes:**\n",
        "\n",
        "| Threshold | TP | FP | TN | FN | FPR | TPR |\n",
        "|-----------|----|----|----|----|-----|-----|\n",
        "| 0 (all positive) | All positives | All negatives | 0 | 0 | 100% | 100% |\n",
        "| 0.5 | Some | Some | Some | Some | Medium | Medium |\n",
        "| 1 (all negative) | 0 | 0 | All negatives | All positives | 0% | 0% |\n",
        "\n",
        "**One-to-One Mapping:**\n",
        "- Each threshold → One confusion matrix\n",
        "- Each confusion matrix → One point on ROC curve\n",
        "- Sweeping threshold → Tracing the ROC curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q35. Summarize when to use each multi-class approach.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Approach Selection Guide:**\n",
        "\n",
        "| Approach | When to Use | When to Avoid |\n",
        "|----------|-------------|---------------|\n",
        "| **Regression** | Never for classification | Always |\n",
        "| **One-vs-Rest** | With inherently binary classifiers (SVM) | When imbalance is severe |\n",
        "| **One-vs-One** | When binary classifier works better | Large K (computational cost) |\n",
        "| **Discriminant** | Most situations | When no direct multi-class support |\n",
        "\n",
        "**Recommended:**\n",
        "\n",
        "1. **Default choice:** Discriminant functions (softmax regression)\n",
        "   - No ambiguity\n",
        "   - Efficient\n",
        "   - Probabilistic output\n",
        "\n",
        "2. **For SVM:** One-vs-Rest or One-vs-One\n",
        "   - SVM is inherently binary\n",
        "   - Need reduction approach\n",
        "\n",
        "3. **For Neural Networks:** Softmax output layer\n",
        "   - Direct K-class output\n",
        "   - End-to-end training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q36. Summarize the key evaluation metrics and when to use each.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Metric Selection Guide:**\n",
        "\n",
        "| Metric | Formula | Use When |\n",
        "|--------|---------|----------|\n",
        "| **Accuracy** | (TP+TN)/Total | Balanced classes |\n",
        "| **Precision** | TP/(TP+FP) | FP is costly |\n",
        "| **Recall/TPR** | TP/(TP+FN) | FN is costly |\n",
        "| **Specificity/TNR** | TN/(TN+FP) | Care about negatives |\n",
        "| **FPR** | FP/(TN+FP) | Evaluating false alarms |\n",
        "| **FNR** | FN/(TP+FN) | Evaluating missed cases |\n",
        "| **AUC** | Area under ROC | Threshold-independent comparison |\n",
        "| **F1 Score** | 2×(Prec×Rec)/(Prec+Rec) | Balance precision and recall |\n",
        "\n",
        "**Curves:**\n",
        "\n",
        "| Curve | Use When |\n",
        "|-------|----------|\n",
        "| **ROC** | Balanced classes, overall comparison |\n",
        "| **PR** | Imbalanced classes, positive class matters |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary of Important Formulas\n",
        "\n",
        "| Concept | Formula |\n",
        "|---------|--------|\n",
        "| **OvR Classifiers** | $K - 1$ |\n",
        "| **OvO Classifiers** | $\\frac{K(K-1)}{2}$ |\n",
        "| **Linear Discriminant** | $s_i(x) = w_i^T x$ |\n",
        "| **Classification Rule** | $y = \\arg\\max_i s_i(x)$ |\n",
        "| **True Positive Rate (TPR)** | $\\frac{TP}{TP + FN}$ |\n",
        "| **True Negative Rate (TNR)** | $\\frac{TN}{TN + FP}$ |\n",
        "| **False Positive Rate (FPR)** | $\\frac{FP}{TN + FP}$ |\n",
        "| **False Negative Rate (FNR)** | $\\frac{FN}{TP + FN}$ |\n",
        "| **Precision** | $\\frac{TP}{TP + FP}$ |\n",
        "| **Recall** | $\\frac{TP}{TP + FN}$ |\n",
        "| **Accuracy** | $\\frac{TP + TN}{TP + TN + FP + FN}$ |\n",
        "| **FPR + TNR** | $= 1$ |\n",
        "| **FNR + TPR** | $= 1$ |\n",
        "\n",
        "---"
      ]
    }
  ]
}