{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 06: Classifier Evaluation\n",
        "## Possible Subjective Exam Questions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Introduction to Classifier Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. Why is error on training data not a good indicator of performance on future data?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Training error is not a good indicator because:\n",
        "\n",
        "1. **New data is different:** Future data will probably not be exactly the same as training data\n",
        "\n",
        "2. **Overfitting problem:** The model might fit the training data too precisely\n",
        "   - It memorizes the training data instead of learning the pattern\n",
        "   - This leads to poor results on new data\n",
        "\n",
        "3. **Training error underestimates test error:** The training error can be very low but test error can be much higher\n",
        "\n",
        "4. **False confidence:** A model with 100% training accuracy might fail badly on real-world data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. What is overfitting? Why is it a problem in machine learning?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Overfitting:**\n",
        "Overfitting occurs when a model fits the training data too precisely, capturing noise and random fluctuations instead of the true underlying pattern.\n",
        "\n",
        "**Why it's a problem:**\n",
        "\n",
        "1. Model performs very well on training data but poorly on new/test data\n",
        "2. The model memorizes instead of generalizes\n",
        "3. It captures noise as if it were a real pattern\n",
        "4. Makes the model unreliable for real-world predictions\n",
        "5. Reduces the practical usefulness of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. What is the difference between training error and test error?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Training Error | Test Error |\n",
        "|----------------|------------|\n",
        "| Error calculated on data used for training | Error calculated on new unseen data |\n",
        "| Easy to calculate | More meaningful for real performance |\n",
        "| Usually lower | Usually higher |\n",
        "| Can be misleading | Better indicator of true performance |\n",
        "| May underestimate true error | Closer to actual real-world error |\n",
        "\n",
        "**Key Point:** Training error often dramatically underestimates the test error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Classifier Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. Explain the components involved in classifier construction.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The main components in classifier construction are:\n",
        "\n",
        "**1. Target Function:**\n",
        "$$f(x, y) = z, \\quad z \\in [-l, +l]$$\n",
        "This is the true function we want to learn.\n",
        "\n",
        "**2. Classifier Hypothesis:**\n",
        "The type of function we assume (linear, quadratic, cubic, etc.)\n",
        "\n",
        "**3. Iterative Optimization:**\n",
        "Finding the best hypothesis by iteratively improving the model\n",
        "\n",
        "**4. Classifier Parameters:**\n",
        "Values learned from data (e.g., slope and intercept for a line)\n",
        "\n",
        "**5. Hyperparameters:**\n",
        "Values set before training (e.g., regularization coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. What is the difference between classifier parameters and hyperparameters? Give examples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Classifier Parameters | Hyperparameters |\n",
        "|----------------------|------------------|\n",
        "| Learned from training data | Set before training begins |\n",
        "| Model finds optimal values | User/developer chooses values |\n",
        "| Internal to the model | External to the model |\n",
        "| **Examples:** slope, intercept, weights | **Examples:** regularization coefficient, learning rate |\n",
        "| Change during training | Fixed during training |\n",
        "| Determined by optimization | Determined by validation |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Evaluation Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. Define Classification Accuracy Rate and Classification Error Rate.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Classification Accuracy Rate:**\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of correct classifications}}{\\text{Total number of test samples}}$$\n",
        "\n",
        "**Classification Error Rate:**\n",
        "\n",
        "$$\\text{Error Rate} = \\frac{\\text{Number of incorrect classifications}}{\\text{Total number of test samples}}$$\n",
        "\n",
        "**Relationship:**\n",
        "$$\\text{Accuracy} + \\text{Error Rate} = 1$$\n",
        "\n",
        "**Example:**\n",
        "If 90 out of 100 samples are correctly classified:\n",
        "- Accuracy = 90/100 = 0.90 or 90%\n",
        "- Error Rate = 10/100 = 0.10 or 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. Explain True Positive, False Positive, True Negative, and False Negative with a medical diagnosis example.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Consider a disease detection classifier:\n",
        "\n",
        "**1. True Positive (TP):**\n",
        "- Classifier says: \"You have the disease\"\n",
        "- Reality: You are actually ill\n",
        "- Correct positive prediction ✓\n",
        "\n",
        "**2. False Positive (FP):**\n",
        "- Classifier says: \"You have the disease\"\n",
        "- Reality: You are actually healthy\n",
        "- Wrong positive prediction ✗ (Type I Error)\n",
        "\n",
        "**3. True Negative (TN):**\n",
        "- Classifier says: \"You don't have the disease\"\n",
        "- Reality: You are actually healthy\n",
        "- Correct negative prediction ✓\n",
        "\n",
        "**4. False Negative (FN):**\n",
        "- Classifier says: \"You don't have the disease\"\n",
        "- Reality: You are actually ill\n",
        "- Wrong negative prediction ✗ (Type II Error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. Draw and explain the Confusion Matrix.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confusion Matrix:**\n",
        "\n",
        "```\n",
        "                    Predicted Class\n",
        "                 Positive    Negative\n",
        "Actual    Positive   TP          FN\n",
        "Class     Negative   FP          TN\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- **TP (True Positive):** Actually positive, predicted positive\n",
        "- **FN (False Negative):** Actually positive, predicted negative\n",
        "- **FP (False Positive):** Actually negative, predicted positive\n",
        "- **TN (True Negative):** Actually negative, predicted negative\n",
        "\n",
        "**Uses:**\n",
        "- Calculate accuracy, precision, recall\n",
        "- Understand types of errors\n",
        "- Compare different classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. Define and explain Sensitivity (True Positive Rate) and Specificity (True Negative Rate).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Sensitivity (True Positive Rate / Recall):**\n",
        "\n",
        "$$\\text{TPR} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "- Probability of positive test result among those who actually have the disease\n",
        "- Also called \"Recall\"\n",
        "- Measures: How good is the model at finding positive cases?\n",
        "\n",
        "**Specificity (True Negative Rate):**\n",
        "\n",
        "$$\\text{TNR} = \\frac{TN}{TN + FP}$$\n",
        "\n",
        "- Probability of negative test result among those who don't have the disease\n",
        "- Measures: How good is the model at avoiding false alarms?\n",
        "\n",
        "**Example:**\n",
        "- High sensitivity: Important in disease screening (don't miss sick people)\n",
        "- High specificity: Important to avoid unnecessary treatments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. Define False Positive Rate and False Negative Rate with formulas.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**False Positive Rate (FPR):**\n",
        "\n",
        "$$\\text{FPR} = \\frac{FP}{TN + FP}$$\n",
        "\n",
        "- Probability of positive test result among those who are actually negative\n",
        "- Also called \"Fall-out\"\n",
        "- Related to specificity: $\\text{FPR} = 1 - \\text{Specificity}$\n",
        "\n",
        "**False Negative Rate (FNR):**\n",
        "\n",
        "$$\\text{FNR} = \\frac{FN}{TP + FN}$$\n",
        "\n",
        "- Probability of negative test result among those who are actually positive\n",
        "- Also called \"Miss Rate\"\n",
        "- Related to sensitivity: $\\text{FNR} = 1 - \\text{Sensitivity}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11. Why might we consider total cost/benefit instead of simple accuracy?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Different errors have different costs in real-world applications:\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Medical Diagnosis:**\n",
        "   - False Negative (missing a disease): Very costly - patient doesn't get treatment\n",
        "   - False Positive (wrong diagnosis): Less costly - extra tests done\n",
        "\n",
        "2. **Spam Detection:**\n",
        "   - False Positive (important email marked as spam): Very costly\n",
        "   - False Negative (spam in inbox): Less costly\n",
        "\n",
        "3. **Fraud Detection:**\n",
        "   - False Negative (missing fraud): Very costly - money lost\n",
        "   - False Positive (blocking valid transaction): Annoying but less costly\n",
        "\n",
        "**Conclusion:** Simple accuracy treats all errors equally, but in practice, some errors are more expensive than others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12. Explain the Bias-Variance Tradeoff with a diagram description.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bias:**\n",
        "- Difference between expected value and true value\n",
        "- Low bias = Model accurately estimates the target function\n",
        "- High bias = Model is too simple (underfitting)\n",
        "\n",
        "**Variance:**\n",
        "- How much the estimate changes when training set varies\n",
        "- Low variance = Model is stable across different training sets\n",
        "- High variance = Model is too sensitive to training data (overfitting)\n",
        "\n",
        "**Tradeoff:**\n",
        "\n",
        "| Model Complexity | Bias | Variance | Training Error | Test Error |\n",
        "|-----------------|------|----------|----------------|------------|\n",
        "| Low (simple) | High | Low | High | High |\n",
        "| Medium (optimal) | Medium | Medium | Medium | Lowest |\n",
        "| High (complex) | Low | High | Low | High |\n",
        "\n",
        "**Goal:** Find the sweet spot where both bias and variance are balanced for minimum test error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13. What happens to training error and test error as model complexity increases?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**As model complexity increases:**\n",
        "\n",
        "**Training Error:**\n",
        "- Continuously decreases\n",
        "- Can become very low or even zero\n",
        "- Model fits training data better and better\n",
        "\n",
        "**Test Error:**\n",
        "- Initially decreases (model learns useful patterns)\n",
        "- Reaches a minimum at optimal complexity\n",
        "- Then increases (overfitting begins)\n",
        "- Model starts fitting noise instead of patterns\n",
        "\n",
        "**Key Insight:**\n",
        "The gap between training error and test error increases with complexity, indicating overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Holdout Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14. Explain the Holdout Method for classifier evaluation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Holdout Method:**\n",
        "\n",
        "A simple evaluation technique for large datasets.\n",
        "\n",
        "**Steps:**\n",
        "1. Randomly split data into two parts:\n",
        "   - Training set: Usually 2/3 of data (67%)\n",
        "   - Test set: Usually 1/3 of data (33%)\n",
        "2. Build classifier using only the training set\n",
        "3. Evaluate classifier using only the test set\n",
        "4. Report the error/accuracy on test set\n",
        "\n",
        "**When to use:**\n",
        "- When you have thousands of examples\n",
        "- When each class has several hundred examples\n",
        "\n",
        "**Limitation:**\n",
        "- For small or unbalanced datasets, samples might not be representative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15. What are the limitations of the simple Holdout Method?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "1. **Randomness:** Results depend on the random split\n",
        "   - Different splits give different results\n",
        "\n",
        "2. **Data waste:** Only 2/3 data used for training\n",
        "   - Less training data means potentially weaker model\n",
        "\n",
        "3. **Small datasets:** May not work well\n",
        "   - Some classes may have very few samples in test set\n",
        "\n",
        "4. **Unbalanced data:** Some classes may be missing\n",
        "   - Rare classes might not appear in test set\n",
        "\n",
        "5. **Single estimate:** Only one error estimate\n",
        "   - No measure of variance/reliability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Handling Unbalanced Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q16. What is unbalanced data? Give real-world examples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Unbalanced Data:**\n",
        "Data where classes have very unequal frequencies - one class has many more samples than others.\n",
        "\n",
        "**Real-world Examples:**\n",
        "\n",
        "| Application | Majority Class | Minority Class |\n",
        "|------------|----------------|----------------|\n",
        "| Attrition Prediction | 97% stay | 3% leave |\n",
        "| Medical Diagnosis | 90% healthy | 10% disease |\n",
        "| eCommerce | 99% don't buy | 1% buy |\n",
        "| Security/Terrorism | >99.99% normal citizens | <0.01% terrorists |\n",
        "| Fraud Detection | 99.9% legitimate | 0.1% fraud |\n",
        "\n",
        "**Problem:**\n",
        "A majority class classifier (always predicts majority class) can achieve 97% accuracy but is completely useless!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17. How do you balance unbalanced data for training?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Balancing Approach for Two Classes:**\n",
        "\n",
        "1. Randomly select desired number of minority class instances\n",
        "2. Add equal number of randomly selected majority class instances\n",
        "3. Train model on this balanced set\n",
        "\n",
        "**For Multiple Classes:**\n",
        "- Ensure each class is represented with approximately equal proportions in both training and test sets\n",
        "\n",
        "**Other Techniques:**\n",
        "- **Oversampling:** Duplicate minority class samples\n",
        "- **Undersampling:** Remove majority class samples\n",
        "- **SMOTE:** Generate synthetic minority samples\n",
        "- **Cost-sensitive learning:** Assign higher cost to minority class errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18. Why can a 97% accurate classifier be useless?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Example: Attrition Prediction**\n",
        "- 97% employees stay, 3% leave\n",
        "\n",
        "**Majority Class Classifier:**\n",
        "- Always predicts \"stay\" for everyone\n",
        "- Accuracy = 97%\n",
        "\n",
        "**Why it's useless:**\n",
        "1. It never identifies any employee who will leave\n",
        "2. The whole purpose was to predict attrition, but it misses 100% of actual attrition cases\n",
        "3. Sensitivity/Recall = 0%\n",
        "4. No actionable insights for HR\n",
        "\n",
        "**Lesson:**\n",
        "High accuracy doesn't mean useful predictions. We need to look at other metrics like sensitivity, specificity, and per-class accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Parameter Tuning and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19. Why should test data never be used for parameter tuning?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Reason:**\n",
        "\n",
        "Test data must be completely unseen by the model until final evaluation because:\n",
        "\n",
        "1. **Data leakage:** Using test data for tuning means the model has \"seen\" it\n",
        "2. **Biased evaluation:** Error estimate will be optimistically biased\n",
        "3. **Overfitting to test set:** Model gets tuned to perform well on that specific test set\n",
        "4. **Invalid results:** The reported accuracy won't reflect true performance on new data\n",
        "\n",
        "**Proper Procedure:**\n",
        "Use three separate sets:\n",
        "- **Training data:** Build the basic model\n",
        "- **Validation data:** Tune hyperparameters\n",
        "- **Test data:** Final evaluation only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20. Explain the purpose of Training, Validation, and Test sets.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Set | Purpose | When Used |\n",
        "|-----|---------|----------|\n",
        "| **Training Set** | Build the basic structure of the model | During model training |\n",
        "| **Validation Set** | Optimize hyperparameters, select best model | During tuning phase |\n",
        "| **Test Set** | Final unbiased evaluation | Only at the end |\n",
        "\n",
        "**Two-Stage Learning:**\n",
        "- Stage 1: Build basic structure using training data\n",
        "- Stage 2: Optimize parameters using validation data\n",
        "- Final: Evaluate using test data\n",
        "\n",
        "**Important:** After evaluation is complete, all data can be used to build the final classifier for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21. What should be done after evaluation is complete?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "After evaluation is complete:\n",
        "\n",
        "1. **Use all data for final model:** Combine training, validation, and test sets\n",
        "2. **Retrain the model:** Build final classifier on complete data\n",
        "3. **Deploy:** Use this model for real predictions\n",
        "\n",
        "**Why?**\n",
        "- Larger training data generally gives better classifiers\n",
        "- Returns diminish, but more data is still helpful\n",
        "- The evaluation phase was just to estimate performance\n",
        "- Final deployed model should use maximum available data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Stratified Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22. What is stratified sampling? Why is it important?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Stratified Sampling:**\n",
        "A sampling technique where samples are selected in the same proportion as they appear in the population.\n",
        "\n",
        "**How it works:**\n",
        "1. Divide population into groups called 'strata' based on a characteristic (usually class label)\n",
        "2. Sample from each stratum proportionally\n",
        "3. Each class is represented with approximately equal proportions in both training and test subsets\n",
        "\n",
        "**Why important:**\n",
        "- Ensures all classes are represented\n",
        "- Reduces variance in error estimates\n",
        "- Especially important for small or unbalanced datasets\n",
        "- More reliable evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Repeated Holdout Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23. Explain the Repeated Holdout Method with algorithm.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Repeated Holdout Method:**\n",
        "Making holdout estimate more reliable by repeating with different subsamples.\n",
        "\n",
        "**Algorithm:**\n",
        "\n",
        "```\n",
        "Input: A = [{x_i, y_i, l_i}] with n samples\n",
        "\n",
        "1. Initialize: Error_Rate = 0, k = 0\n",
        "\n",
        "2. Repeat for k = 1 to 10:\n",
        "   a. Randomly sample n/4 samples for training and test\n",
        "   b. Train the classifier\n",
        "   c. Compute error rate e(k) on test set\n",
        "   d. Error_Rate = Error_Rate + e(k)\n",
        "   e. k = k + 1\n",
        "\n",
        "3. Return: Error_Rate / k\n",
        "```\n",
        "\n",
        "**Advantage:** More reliable than single holdout\n",
        "\n",
        "**Limitation:** Test sets overlap between iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24. What is the main limitation of Repeated Holdout Method?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Main Limitation:**\n",
        "The different test sets overlap with each other.\n",
        "\n",
        "**Problem with overlapping:**\n",
        "- Same instances appear in multiple test sets\n",
        "- Error estimates are not fully independent\n",
        "- Some instances are tested multiple times, others never\n",
        "\n",
        "**Solution:**\n",
        "Cross-validation avoids overlapping test sets by ensuring each instance is tested exactly once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25. Explain K-Fold Cross-Validation with diagram description.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**K-Fold Cross-Validation:**\n",
        "\n",
        "**Steps:**\n",
        "1. Split data into k subsets of equal size\n",
        "2. For each iteration i (from 1 to k):\n",
        "   - Use subset i as test set\n",
        "   - Use remaining (k-1) subsets as training set\n",
        "   - Train model and calculate error\n",
        "3. Average all k error estimates\n",
        "\n",
        "**Example with k=5:**\n",
        "```\n",
        "Iteration 1: [TEST] [Train] [Train] [Train] [Train]\n",
        "Iteration 2: [Train] [TEST] [Train] [Train] [Train]\n",
        "Iteration 3: [Train] [Train] [TEST] [Train] [Train]\n",
        "Iteration 4: [Train] [Train] [Train] [TEST] [Train]\n",
        "Iteration 5: [Train] [Train] [Train] [Train] [TEST]\n",
        "```\n",
        "\n",
        "**Advantage:** No overlapping test sets - each instance is tested exactly once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26. Why is 10-fold cross-validation considered the standard method?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**10-Fold Cross-Validation is standard because:**\n",
        "\n",
        "1. **Empirical evidence:** Extensive experiments have shown k=10 gives the best balance\n",
        "\n",
        "2. **Good bias-variance tradeoff:**\n",
        "   - Low k (like 2): High bias, low variance\n",
        "   - High k (like n): Low bias, high variance\n",
        "   - k=10: Good balance\n",
        "\n",
        "3. **Sufficient training data:** 90% data used for training in each fold\n",
        "\n",
        "4. **Reliable test estimate:** 10 different test sets provide stable average\n",
        "\n",
        "5. **Computationally feasible:** Not too many iterations\n",
        "\n",
        "**Best practice:** Stratified ten-fold cross-validation (stratification reduces variance further)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27. What is Stratified Cross-Validation? What are its benefits?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Stratified Cross-Validation:**\n",
        "Cross-validation where each fold maintains the same class distribution as the original dataset.\n",
        "\n",
        "**How it works:**\n",
        "1. Before splitting, stratify the data by class\n",
        "2. Ensure each fold has proportional representation of all classes\n",
        "3. Then perform k-fold cross-validation\n",
        "\n",
        "**Benefits:**\n",
        "1. Reduces variance in error estimates\n",
        "2. More reliable for unbalanced datasets\n",
        "3. Each test fold is representative of the whole dataset\n",
        "4. Avoids having folds with missing classes\n",
        "\n",
        "**Best Practice:**\n",
        "Repeated stratified cross-validation (e.g., 10-fold repeated 10 times) for most reliable estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Leave-One-Out Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28. Explain Leave-One-Out Cross-Validation (LOOCV). When is it used?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Leave-One-Out Cross-Validation (LOOCV):**\n",
        "\n",
        "A special case of k-fold cross-validation where k = n (number of samples).\n",
        "\n",
        "**How it works:**\n",
        "- For n training instances, build classifier n times\n",
        "- Each time:\n",
        "  - Leave one instance out for testing\n",
        "  - Train on remaining (n-1) instances\n",
        "  - Test on the single left-out instance\n",
        "- Average all n results\n",
        "\n",
        "**When to use:**\n",
        "- Acute shortage of labeled data (e.g., medical domain)\n",
        "- Need to use maximum data for training\n",
        "\n",
        "**Advantages:**\n",
        "- Makes best use of limited data\n",
        "- No random subsampling involved\n",
        "- Deterministic results\n",
        "\n",
        "**Disadvantages:**\n",
        "- Very computationally expensive (train n models)\n",
        "- High variance in estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q29. Compare K-Fold Cross-Validation and Leave-One-Out Cross-Validation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect | K-Fold CV | LOOCV |\n",
        "|--------|-----------|-------|\n",
        "| Number of folds | k (typically 10) | n (number of samples) |\n",
        "| Training set size | (k-1)/k of data | (n-1)/n of data |\n",
        "| Test set size | 1/k of data | 1 sample |\n",
        "| Computational cost | Train k models | Train n models |\n",
        "| Variance | Lower | Higher |\n",
        "| Bias | Slightly higher | Lower |\n",
        "| Best for | Medium-large datasets | Very small datasets |\n",
        "| Random subsampling | Yes | No |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 12: Bootstrap Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q30. Explain the Bootstrap method for evaluation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bootstrap Method:**\n",
        "\n",
        "Uses sampling **with replacement** to form training sets.\n",
        "\n",
        "**How it works:**\n",
        "1. From dataset of n instances, sample n times **with replacement**\n",
        "2. This forms a new training set of n instances (some repeated)\n",
        "3. Use instances NOT in the new training set for testing\n",
        "4. Repeat multiple times and average results\n",
        "\n",
        "**Key difference from Cross-Validation:**\n",
        "- Cross-validation: Sampling **without** replacement\n",
        "- Bootstrap: Sampling **with** replacement\n",
        "\n",
        "**Note:** Same instance can appear multiple times in training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q31. What is the 0.632 Bootstrap? Derive the probability.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**0.632 Bootstrap:**\n",
        "\n",
        "Named because approximately 63.2% of instances appear in the training set.\n",
        "\n",
        "**Derivation:**\n",
        "\n",
        "For a single instance:\n",
        "- Probability of NOT being picked in one draw = $1 - \\frac{1}{n}$\n",
        "- Probability of NOT being picked in n draws = $\\left(1 - \\frac{1}{n}\\right)^n$\n",
        "\n",
        "As n becomes large:\n",
        "$$\\left(1 - \\frac{1}{n}\\right)^n \\approx e^{-1} \\approx 0.368$$\n",
        "\n",
        "**Therefore:**\n",
        "- Probability of being in test set = 0.368 (36.8%)\n",
        "- Probability of being in training set = 1 - 0.368 = 0.632 (63.2%)\n",
        "\n",
        "**Meaning:** Training data will contain approximately 63.2% of the original instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q32. What is the difference between sampling with replacement and without replacement?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Sampling Without Replacement (Cross-Validation):**\n",
        "- Once an instance is selected, it cannot be selected again\n",
        "- Each instance appears at most once in training set\n",
        "- Like drawing cards without putting them back\n",
        "\n",
        "**Sampling With Replacement (Bootstrap):**\n",
        "- After selecting an instance, it's \"put back\" and can be selected again\n",
        "- Same instance can appear multiple times in training set\n",
        "- Like drawing cards and putting them back\n",
        "\n",
        "**Example:**\n",
        "Dataset: {A, B, C, D, E}\n",
        "\n",
        "Without replacement: {A, C, E} - each appears once\n",
        "With replacement: {A, A, C, E, E} - some appear multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 13: Numerical Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q33. Given the following confusion matrix, calculate Accuracy, Sensitivity, Specificity, FPR, and FNR.\n",
        "\n",
        "```\n",
        "                Predicted\n",
        "              Pos    Neg\n",
        "Actual  Pos   80     20\n",
        "        Neg   10     90\n",
        "```\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "From the confusion matrix:\n",
        "- TP = 80, FN = 20\n",
        "- FP = 10, TN = 90\n",
        "- Total = 200\n",
        "\n",
        "**Accuracy:**\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{80 + 90}{200} = \\frac{170}{200} = 0.85 = 85\\%$$\n",
        "\n",
        "**Sensitivity (TPR):**\n",
        "$$\\text{TPR} = \\frac{TP}{TP + FN} = \\frac{80}{80 + 20} = \\frac{80}{100} = 0.80 = 80\\%$$\n",
        "\n",
        "**Specificity (TNR):**\n",
        "$$\\text{TNR} = \\frac{TN}{TN + FP} = \\frac{90}{90 + 10} = \\frac{90}{100} = 0.90 = 90\\%$$\n",
        "\n",
        "**False Positive Rate:**\n",
        "$$\\text{FPR} = \\frac{FP}{TN + FP} = \\frac{10}{100} = 0.10 = 10\\%$$\n",
        "\n",
        "**False Negative Rate:**\n",
        "$$\\text{FNR} = \\frac{FN}{TP + FN} = \\frac{20}{100} = 0.20 = 20\\%$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q34. In 5-fold cross-validation with 100 samples, how many samples are in each training and test set?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:**\n",
        "- Total samples = 100\n",
        "- Number of folds (k) = 5\n",
        "\n",
        "**Calculation:**\n",
        "\n",
        "Each fold contains:\n",
        "$$\\frac{100}{5} = 20 \\text{ samples}$$\n",
        "\n",
        "In each iteration:\n",
        "- **Test set size** = 1 fold = 20 samples\n",
        "- **Training set size** = 4 folds = 80 samples\n",
        "\n",
        "**Summary:**\n",
        "- 5 iterations total\n",
        "- Each iteration: 80 training, 20 testing\n",
        "- Each sample is tested exactly once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q35. If a dataset has 50 samples, how many models are trained in Leave-One-Out Cross-Validation?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:** n = 50 samples\n",
        "\n",
        "In LOOCV:\n",
        "- k = n = 50 folds\n",
        "- Each fold has exactly 1 sample for testing\n",
        "\n",
        "**Answer:**\n",
        "- **50 models** are trained\n",
        "- Each model is trained on 49 samples and tested on 1 sample\n",
        "- Total test predictions = 50\n",
        "- Average of 50 error values gives final error estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q36. Calculate the probability that a specific instance is NOT selected in any of the n bootstrap samples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Probability Calculation:**\n",
        "\n",
        "For a dataset with n instances:\n",
        "\n",
        "Probability of NOT selecting a specific instance in one draw:\n",
        "$$P(\\text{not selected in 1 draw}) = 1 - \\frac{1}{n}$$\n",
        "\n",
        "Probability of NOT selecting in n draws (n samples with replacement):\n",
        "$$P(\\text{not selected in n draws}) = \\left(1 - \\frac{1}{n}\\right)^n$$\n",
        "\n",
        "As $n \\to \\infty$:\n",
        "$$\\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n}\\right)^n = e^{-1} \\approx 0.368$$\n",
        "\n",
        "**Answer:** Approximately **36.8%** probability of not being selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 14: Conceptual Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q37. Compare all evaluation methods: Holdout, Repeated Holdout, K-Fold CV, LOOCV, and Bootstrap.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Method | Training % | Test % | Overlap | Best For |\n",
        "|--------|------------|--------|---------|----------|\n",
        "| Holdout | 67% | 33% | N/A | Large data |\n",
        "| Repeated Holdout | Variable | Variable | Yes | Medium data |\n",
        "| K-Fold CV | (k-1)/k | 1/k | No | Most cases |\n",
        "| LOOCV | (n-1)/n | 1/n | No | Very small data |\n",
        "| Bootstrap | ~63.2% | ~36.8% | In training | Statistical estimates |\n",
        "\n",
        "**Recommendations:**\n",
        "- **Large data:** Simple holdout with train/validation/test split\n",
        "- **Medium data:** 10-fold stratified cross-validation\n",
        "- **Small data:** LOOCV or repeated stratified cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q38. What is the relationship between model complexity and training/test error?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**As model complexity increases:**\n",
        "\n",
        "| Complexity | Training Error | Test Error | Phenomenon |\n",
        "|------------|---------------|------------|------------|\n",
        "| Too Low | High | High | Underfitting (High Bias) |\n",
        "| Optimal | Medium | Lowest | Good Generalization |\n",
        "| Too High | Very Low | High | Overfitting (High Variance) |\n",
        "\n",
        "**Key Observations:**\n",
        "1. Training error always decreases with complexity\n",
        "2. Test error has a U-shape\n",
        "3. The gap between them indicates overfitting\n",
        "4. Goal: Find complexity where test error is minimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q39. Summarize the key points to remember for classifier evaluation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Key Summary Points:**\n",
        "\n",
        "1. **Use appropriate data splits:**\n",
        "   - Large data: Train, Validation, Test sets\n",
        "   - Small data: Cross-validation\n",
        "\n",
        "2. **Balance unbalanced data:**\n",
        "   - Don't let majority class dominate\n",
        "   - Stratify samples proportionally\n",
        "\n",
        "3. **Never use test data for parameter tuning:**\n",
        "   - Use separate validation data\n",
        "\n",
        "4. **Most Important: Avoid Overfitting:**\n",
        "   - Monitor training vs test error gap\n",
        "   - Use regularization\n",
        "   - Don't make model too complex\n",
        "\n",
        "5. **After evaluation, use all data for final model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary of Important Formulas\n",
        "\n",
        "| Metric | Formula |\n",
        "|--------|--------|\n",
        "| Accuracy | $\\frac{TP + TN}{TP + TN + FP + FN}$ |\n",
        "| Error Rate | $\\frac{FP + FN}{TP + TN + FP + FN}$ |\n",
        "| Sensitivity (TPR/Recall) | $\\frac{TP}{TP + FN}$ |\n",
        "| Specificity (TNR) | $\\frac{TN}{TN + FP}$ |\n",
        "| False Positive Rate | $\\frac{FP}{TN + FP}$ |\n",
        "| False Negative Rate | $\\frac{FN}{TP + FN}$ |\n",
        "| Bootstrap Probability | $\\left(1 - \\frac{1}{n}\\right)^n \\approx 0.368$ |\n",
        "\n",
        "---"
      ]
    }
  ]
}