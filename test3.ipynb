{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 09: Logistic Regression\n",
        "## Possible Subjective Exam Questions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Introduction - From Regression to Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. Why can't we directly use linear regression for classification tasks? Explain with an example.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Problem with Linear Regression for Classification:**\n",
        "\n",
        "Linear regression is not suitable for classification because:\n",
        "\n",
        "1. **Predictions can go outside [0,1]:** Linear regression can predict values like -0.5 or 1.5, which don't make sense as probabilities\n",
        "\n",
        "2. **Not bounded:** The output range is $(-\\infty, +\\infty)$ but we need values between 0 and 1\n",
        "\n",
        "3. **Sensitive to outliers:** Extreme values can shift the regression line badly\n",
        "\n",
        "**Example:**\n",
        "- Medical diagnosis: Stroke ($y=0$) vs Drug Overdose ($y=1$)\n",
        "- If we use linear regression and predict $\\hat{y} = 1.3$, what does this mean?\n",
        "- It cannot be interpreted as probability since probability must be between 0 and 1\n",
        "- Similarly, $\\hat{y} = -0.2$ doesn't make sense as a probability\n",
        "\n",
        "**Solution:** Use Logistic Regression which outputs probabilities in [0,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. What is binary classification? Give a real-world example with proper encoding.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Binary Classification:**\n",
        "A classification problem where there are exactly two possible classes/outcomes.\n",
        "\n",
        "**Encoding:**\n",
        "- Class 0: Negative class\n",
        "- Class 1: Positive class\n",
        "\n",
        "**Real-World Example: Emergency Room Diagnosis**\n",
        "\n",
        "| Patient Symptoms | Actual Condition | Encoding |\n",
        "|-----------------|------------------|----------|\n",
        "| Symptom set A | Stroke | $y = 0$ |\n",
        "| Symptom set B | Drug Overdose | $y = 1$ |\n",
        "\n",
        "**Other Examples:**\n",
        "- Email: Spam (1) vs Not Spam (0)\n",
        "- Medical: Disease Present (1) vs Absent (0)\n",
        "- Finance: Fraud (1) vs Legitimate (0)\n",
        "- Image: Cat (1) vs Dog (0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. If we use linear regression for binary classification with threshold 0, what are the potential problems?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Approach:**\n",
        "- Fit linear regression to binary response\n",
        "- Classify as Class 1 if $\\hat{y} > 0$, otherwise Class 0\n",
        "\n",
        "**Problems:**\n",
        "\n",
        "1. **Unbounded predictions:**\n",
        "   - $\\hat{y}$ can be any value from $-\\infty$ to $+\\infty$\n",
        "   - Cannot interpret as probability\n",
        "\n",
        "2. **Sensitive to outliers:**\n",
        "   - Adding extreme points can shift the line dramatically\n",
        "   - Changes classification for many points\n",
        "\n",
        "3. **Poor probability estimates:**\n",
        "   - If $\\hat{y} = 5$, we can't say probability is 5\n",
        "   - If $\\hat{y} = -2$, we can't say probability is -2\n",
        "\n",
        "4. **Non-optimal decision boundary:**\n",
        "   - Linear regression minimizes squared error, not classification error\n",
        "   - The objective is wrong for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: The Logistic (Sigmoid) Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. Define the Logistic (Sigmoid) function and explain its properties.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "$$s(a) = \\frac{1}{1 + e^{-a}} = \\frac{e^a}{1 + e^a}$$\n",
        "\n",
        "**Properties:**\n",
        "\n",
        "| Property | Explanation |\n",
        "|----------|-------------|\n",
        "| **Output Range** | Always between 0 and 1: $s(a) \\in (0, 1)$ |\n",
        "| **When $a \\gg 0$** | $s(a) \\approx 1$ (approaches 1) |\n",
        "| **When $a \\ll 0$** | $s(a) \\approx 0$ (approaches 0) |\n",
        "| **When $a = 0$** | $s(0) = \\frac{1}{1+1} = 0.5$ |\n",
        "| **Monotonic** | Always increasing |\n",
        "| **Symmetric** | $s(-a) = 1 - s(a)$ |\n",
        "| **Differentiable** | Smooth curve, easy to compute gradient |\n",
        "\n",
        "**Why Useful:**\n",
        "- Converts any real number to a probability\n",
        "- Suitable for modeling binary outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. Calculate the sigmoid function value for $a = 0$, $a = 2$, and $a = -2$.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Formula:** $s(a) = \\frac{1}{1 + e^{-a}}$\n",
        "\n",
        "**Case 1: $a = 0$**\n",
        "$$s(0) = \\frac{1}{1 + e^{0}} = \\frac{1}{1 + 1} = \\frac{1}{2} = 0.5$$\n",
        "\n",
        "**Case 2: $a = 2$**\n",
        "$$s(2) = \\frac{1}{1 + e^{-2}} = \\frac{1}{1 + 0.135} = \\frac{1}{1.135} \\approx 0.88$$\n",
        "\n",
        "**Case 3: $a = -2$**\n",
        "$$s(-2) = \\frac{1}{1 + e^{2}} = \\frac{1}{1 + 7.389} = \\frac{1}{8.389} \\approx 0.12$$\n",
        "\n",
        "**Observation:**\n",
        "- $s(2) + s(-2) = 0.88 + 0.12 = 1$ (symmetry property)\n",
        "- Positive input → probability > 0.5\n",
        "- Negative input → probability < 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. Prove that the sigmoid function satisfies: $s(-a) = 1 - s(a)$\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**To Prove:** $s(-a) = 1 - s(a)$\n",
        "\n",
        "**Proof:**\n",
        "\n",
        "Starting with the left side:\n",
        "$$s(-a) = \\frac{1}{1 + e^{-(-a)}} = \\frac{1}{1 + e^{a}}$$\n",
        "\n",
        "Now, let's compute $1 - s(a)$:\n",
        "$$1 - s(a) = 1 - \\frac{1}{1 + e^{-a}}$$\n",
        "\n",
        "$$= \\frac{(1 + e^{-a}) - 1}{1 + e^{-a}}$$\n",
        "\n",
        "$$= \\frac{e^{-a}}{1 + e^{-a}}$$\n",
        "\n",
        "Multiply numerator and denominator by $e^a$:\n",
        "$$= \\frac{e^{-a} \\cdot e^a}{(1 + e^{-a}) \\cdot e^a} = \\frac{1}{e^a + 1} = \\frac{1}{1 + e^a}$$\n",
        "\n",
        "This equals $s(-a)$. Hence proved: $s(-a) = 1 - s(a)$ ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. Why is the sigmoid function preferred over other functions for binary classification?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Reasons for Using Sigmoid:**\n",
        "\n",
        "1. **Bounded Output:**\n",
        "   - Always outputs values in $(0, 1)$\n",
        "   - Directly interpretable as probability\n",
        "\n",
        "2. **Smooth and Differentiable:**\n",
        "   - Can compute gradients easily\n",
        "   - Enables gradient descent optimization\n",
        "\n",
        "3. **Nice Derivative:**\n",
        "   - $\\frac{ds}{da} = s(a)(1 - s(a))$\n",
        "   - Simple formula in terms of the function itself\n",
        "\n",
        "4. **Probabilistic Interpretation:**\n",
        "   - Derived from log-odds (logit function)\n",
        "   - Has statistical foundation\n",
        "\n",
        "5. **Decision Boundary at 0.5:**\n",
        "   - When $a = 0$, output is exactly 0.5\n",
        "   - Natural threshold for classification\n",
        "\n",
        "6. **Monotonically Increasing:**\n",
        "   - Higher input always gives higher probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. Write the complete logistic regression model. Explain each component.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Logistic Regression Model:**\n",
        "\n",
        "**Step 1: Linear Combination**\n",
        "$$a = \\varphi^T \\theta$$\n",
        "\n",
        "**Step 2: Apply Sigmoid**\n",
        "$$P(y=1|\\varphi) = s(\\varphi^T \\theta) = \\frac{1}{1 + e^{-\\varphi^T \\theta}}$$\n",
        "\n",
        "**Components Explained:**\n",
        "\n",
        "| Symbol | Meaning | Dimension |\n",
        "|--------|---------|------------|\n",
        "| $\\varphi$ | Feature vector (input) | $d \\times 1$ |\n",
        "| $\\theta$ | Parameter vector (weights) | $d \\times 1$ |\n",
        "| $\\varphi^T \\theta$ | Linear combination (score) | scalar |\n",
        "| $s(\\cdot)$ | Sigmoid function | maps to $(0,1)$ |\n",
        "| $P(y=1|\\varphi)$ | Probability of class 1 given input | $(0,1)$ |\n",
        "\n",
        "**Also:**\n",
        "$$P(y=0|\\varphi) = 1 - P(y=1|\\varphi) = \\frac{e^{-\\varphi^T\\theta}}{1 + e^{-\\varphi^T\\theta}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. Explain the classification rule in logistic regression. Why is threshold 0.5 used?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Classification Rule:**\n",
        "\n",
        "$$\\text{Predicted Class} = \\begin{cases} 1 & \\text{if } s(\\varphi^T\\theta) \\geq 0.5 \\\\ 0 & \\text{if } s(\\varphi^T\\theta) < 0.5 \\end{cases}$$\n",
        "\n",
        "**Equivalent Rule:**\n",
        "$$\\text{Predicted Class} = \\begin{cases} 1 & \\text{if } \\varphi^T\\theta \\geq 0 \\\\ 0 & \\text{if } \\varphi^T\\theta < 0 \\end{cases}$$\n",
        "\n",
        "**Why 0.5 is Used:**\n",
        "\n",
        "1. **Natural threshold:** When $s(a) = 0.5$, both classes are equally likely\n",
        "\n",
        "2. **At $a = 0$:** $s(0) = 0.5$ exactly\n",
        "\n",
        "3. **Maximum uncertainty point:** The point where we're most unsure\n",
        "\n",
        "4. **Equal error cost assumption:** Assumes misclassifying Class 0 as Class 1 has same cost as the reverse\n",
        "\n",
        "**Note:** In practice, threshold can be adjusted based on:\n",
        "- Cost of different errors\n",
        "- Class imbalance\n",
        "- Application requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. Show that the classification rule $s(\\varphi^T\\theta) \\geq 0.5$ is equivalent to $\\varphi^T\\theta \\geq 0$.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**To Show:** $s(\\varphi^T\\theta) \\geq 0.5 \\Leftrightarrow \\varphi^T\\theta \\geq 0$\n",
        "\n",
        "**Proof:**\n",
        "\n",
        "Let $a = \\varphi^T\\theta$\n",
        "\n",
        "We need to show: $s(a) \\geq 0.5 \\Leftrightarrow a \\geq 0$\n",
        "\n",
        "Starting with $s(a) \\geq 0.5$:\n",
        "\n",
        "$$\\frac{1}{1 + e^{-a}} \\geq 0.5$$\n",
        "\n",
        "$$1 \\geq 0.5(1 + e^{-a})$$\n",
        "\n",
        "$$1 \\geq 0.5 + 0.5e^{-a}$$\n",
        "\n",
        "$$0.5 \\geq 0.5e^{-a}$$\n",
        "\n",
        "$$1 \\geq e^{-a}$$\n",
        "\n",
        "Taking natural log (which preserves inequality):\n",
        "\n",
        "$$\\ln(1) \\geq -a$$\n",
        "\n",
        "$$0 \\geq -a$$\n",
        "\n",
        "$$a \\geq 0$$\n",
        "\n",
        "Hence proved: $\\varphi^T\\theta \\geq 0$ ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11. What is the decision boundary in logistic regression? Why is it linear?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Decision Boundary:**\n",
        "The set of points where the classifier is uncertain (probability = 0.5).\n",
        "\n",
        "**Mathematical Definition:**\n",
        "$$\\text{Decision Boundary: } \\varphi^T\\theta = 0$$\n",
        "\n",
        "Or equivalently:\n",
        "$$\\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d = 0$$\n",
        "\n",
        "**Why It's Linear:**\n",
        "\n",
        "1. The equation $\\varphi^T\\theta = 0$ is a linear equation in the features\n",
        "\n",
        "2. In 2D: $\\theta_1 x_1 + \\theta_2 x_2 + \\theta_0 = 0$ is a straight line\n",
        "\n",
        "3. In 3D: It's a plane\n",
        "\n",
        "4. In higher dimensions: It's a hyperplane\n",
        "\n",
        "**Geometric Interpretation:**\n",
        "- Points on one side: $\\varphi^T\\theta > 0$ → Class 1\n",
        "- Points on other side: $\\varphi^T\\theta < 0$ → Class 0\n",
        "- Points on boundary: $\\varphi^T\\theta = 0$ → Uncertain\n",
        "\n",
        "**Note:** Logistic regression is a **linear classifier** because its decision boundary is linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Cost Function (Binary Cross-Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12. Why can't we use Mean Squared Error (MSE) as the cost function for logistic regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Problem with MSE for Logistic Regression:**\n",
        "\n",
        "If we use MSE:\n",
        "$$J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}(s(\\varphi^T\\theta) - y)^2$$\n",
        "\n",
        "**Issues:**\n",
        "\n",
        "1. **Non-convex:**\n",
        "   - The function becomes non-convex due to sigmoid\n",
        "   - Has many local minima\n",
        "   - Gradient descent may get stuck\n",
        "\n",
        "2. **Small gradients:**\n",
        "   - When sigmoid output is near 0 or 1\n",
        "   - Gradient becomes very small\n",
        "   - Learning becomes very slow\n",
        "\n",
        "3. **Not probabilistically motivated:**\n",
        "   - Doesn't align with maximum likelihood principle\n",
        "   - Cross-entropy has proper statistical foundation\n",
        "\n",
        "**Solution:** Use Binary Cross-Entropy (Log Loss) which:\n",
        "- Is convex\n",
        "- Has better gradients\n",
        "- Is derived from maximum likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13. Define the Log Loss (Binary Cross-Entropy) cost function. Explain intuitively why it works.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Log Loss Formula:**\n",
        "\n",
        "For a single sample $(\\varphi, y)$:\n",
        "\n",
        "$$f(\\theta) = \\begin{cases} -\\ln(\\pi) & \\text{if } y = 1 \\\\ -\\ln(1-\\pi) & \\text{if } y = 0 \\end{cases}$$\n",
        "\n",
        "Where $\\pi = P(y=1|\\varphi) = s(\\varphi^T\\theta)$\n",
        "\n",
        "**Unified Formula:**\n",
        "$$f(\\theta) = -[y \\ln(\\pi) + (1-y) \\ln(1-\\pi)]$$\n",
        "\n",
        "**Intuitive Explanation:**\n",
        "\n",
        "| Actual $y$ | Predicted $\\pi$ | $-\\ln(\\pi)$ or $-\\ln(1-\\pi)$ | Cost |\n",
        "|------------|-----------------|------------------------------|------|\n",
        "| 1 | $\\approx 1$ | $-\\ln(1) = 0$ | Low ✓ |\n",
        "| 1 | $\\approx 0$ | $-\\ln(0) = \\infty$ | High (penalty) |\n",
        "| 0 | $\\approx 0$ | $-\\ln(1) = 0$ | Low ✓ |\n",
        "| 0 | $\\approx 1$ | $-\\ln(0) = \\infty$ | High (penalty) |\n",
        "\n",
        "**Key Insight:** Confident wrong predictions are heavily penalized!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14. Write the complete cost function for logistic regression over N training samples.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Complete Cost Function:**\n",
        "\n",
        "$$J(\\theta) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\ln(\\pi^{(i)}) + (1 - y^{(i)}) \\ln(1 - \\pi^{(i)}) \\right]$$\n",
        "\n",
        "Where:\n",
        "- $N$ = number of training samples\n",
        "- $y^{(i)}$ = actual label for sample $i$ (0 or 1)\n",
        "- $\\pi^{(i)} = s(\\varphi^{(i)T}\\theta)$ = predicted probability for sample $i$\n",
        "\n",
        "**Expanded Form:**\n",
        "$$J(\\theta) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\ln\\left(\\frac{1}{1+e^{-\\varphi^{(i)T}\\theta}}\\right) + (1 - y^{(i)}) \\ln\\left(\\frac{e^{-\\varphi^{(i)T}\\theta}}{1+e^{-\\varphi^{(i)T}\\theta}}\\right) \\right]$$\n",
        "\n",
        "**Alternative with Average:**\n",
        "$$J(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\left[ y^{(i)} \\ln(\\pi^{(i)}) + (1 - y^{(i)}) \\ln(1 - \\pi^{(i)}) \\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15. Calculate the cost for the following cases:\n",
        "- Case A: $y = 1$, $\\pi = 0.9$\n",
        "- Case B: $y = 1$, $\\pi = 0.1$\n",
        "- Case C: $y = 0$, $\\pi = 0.2$\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Formula:** $f(\\theta) = -[y \\ln(\\pi) + (1-y) \\ln(1-\\pi)]$\n",
        "\n",
        "**Case A: $y = 1$, $\\pi = 0.9$**\n",
        "$$f = -[1 \\cdot \\ln(0.9) + 0 \\cdot \\ln(0.1)]$$\n",
        "$$= -\\ln(0.9) = -(-0.105) = 0.105$$\n",
        "\n",
        "**Interpretation:** Low cost - correct confident prediction ✓\n",
        "\n",
        "**Case B: $y = 1$, $\\pi = 0.1$**\n",
        "$$f = -[1 \\cdot \\ln(0.1) + 0 \\cdot \\ln(0.9)]$$\n",
        "$$= -\\ln(0.1) = -(-2.303) = 2.303$$\n",
        "\n",
        "**Interpretation:** High cost - wrong confident prediction ✗\n",
        "\n",
        "**Case C: $y = 0$, $\\pi = 0.2$**\n",
        "$$f = -[0 \\cdot \\ln(0.2) + 1 \\cdot \\ln(0.8)]$$\n",
        "$$= -\\ln(0.8) = -(-0.223) = 0.223$$\n",
        "\n",
        "**Interpretation:** Low cost - correct prediction ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q16. Why is the cross-entropy cost function convex for logistic regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Convexity of Cross-Entropy:**\n",
        "\n",
        "The cost function $J(\\theta)$ is convex because:\n",
        "\n",
        "1. **Negative log-likelihood:**\n",
        "   - Cross-entropy is the negative log of likelihood\n",
        "   - Log of sigmoid products leads to a convex function\n",
        "\n",
        "2. **Second derivative is positive:**\n",
        "   - The Hessian matrix is positive semi-definite\n",
        "   - This guarantees convexity\n",
        "\n",
        "3. **Composition of convex functions:**\n",
        "   - $-\\ln(x)$ is convex for $x > 0$\n",
        "   - Sigmoid output is always positive\n",
        "   - Sum of convex functions is convex\n",
        "\n",
        "**Benefits of Convexity:**\n",
        "1. Only one minimum exists (global minimum)\n",
        "2. No local minima to get stuck in\n",
        "3. Gradient descent will find the optimal solution\n",
        "4. Solution is unique\n",
        "\n",
        "**Note:** However, there is no closed-form solution like in linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Gradient of the Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17. Derive the derivative of the sigmoid function.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**To Find:** $\\frac{\\partial s(a)}{\\partial a}$ where $s(a) = \\frac{1}{1 + e^{-a}}$\n",
        "\n",
        "**Derivation:**\n",
        "\n",
        "Let $s(a) = (1 + e^{-a})^{-1}$\n",
        "\n",
        "Using chain rule:\n",
        "$$\\frac{ds}{da} = -1 \\cdot (1 + e^{-a})^{-2} \\cdot \\frac{d}{da}(1 + e^{-a})$$\n",
        "\n",
        "$$= -(1 + e^{-a})^{-2} \\cdot (-e^{-a})$$\n",
        "\n",
        "$$= \\frac{e^{-a}}{(1 + e^{-a})^2}$$\n",
        "\n",
        "Now, notice that:\n",
        "$$s(a) = \\frac{1}{1 + e^{-a}}$$\n",
        "$$1 - s(a) = \\frac{e^{-a}}{1 + e^{-a}}$$\n",
        "\n",
        "Therefore:\n",
        "$$\\frac{ds}{da} = \\frac{1}{1 + e^{-a}} \\cdot \\frac{e^{-a}}{1 + e^{-a}} = s(a) \\cdot (1 - s(a))$$\n",
        "\n",
        "**Final Result:**\n",
        "$$\\boxed{\\frac{\\partial s(a)}{\\partial a} = s(a)(1 - s(a))}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18. Derive the gradient of the logistic regression cost function.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Cost Function:**\n",
        "$$J(\\theta) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\ln(\\pi^{(i)}) + (1 - y^{(i)}) \\ln(1 - \\pi^{(i)}) \\right]$$\n",
        "\n",
        "Where $\\pi^{(i)} = s(\\varphi^{(i)T}\\theta)$\n",
        "\n",
        "**Step 1:** For a single sample, the cost is:\n",
        "$$f = -[y \\ln(\\pi) + (1-y) \\ln(1-\\pi)]$$\n",
        "\n",
        "**Step 2:** Compute $\\frac{\\partial f}{\\partial \\theta}$ using chain rule:\n",
        "$$\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial \\pi} \\cdot \\frac{\\partial \\pi}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\theta}$$\n",
        "\n",
        "Where $a = \\varphi^T\\theta$\n",
        "\n",
        "**Step 3:** Compute each part:\n",
        "- $\\frac{\\partial f}{\\partial \\pi} = -\\frac{y}{\\pi} + \\frac{1-y}{1-\\pi}$\n",
        "- $\\frac{\\partial \\pi}{\\partial a} = \\pi(1-\\pi)$\n",
        "- $\\frac{\\partial a}{\\partial \\theta} = \\varphi$\n",
        "\n",
        "**Step 4:** Combine:\n",
        "$$\\frac{\\partial f}{\\partial \\theta} = \\left(-\\frac{y}{\\pi} + \\frac{1-y}{1-\\pi}\\right) \\cdot \\pi(1-\\pi) \\cdot \\varphi$$\n",
        "\n",
        "$$= (-y(1-\\pi) + (1-y)\\pi) \\cdot \\varphi$$\n",
        "\n",
        "$$= (\\pi - y) \\cdot \\varphi$$\n",
        "\n",
        "**Final Gradient:**\n",
        "$$\\boxed{\\nabla_\\theta J(\\theta) = \\sum_{i=1}^{N} \\varphi^{(i)} \\cdot (\\pi^{(i)} - y^{(i)})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19. Write the gradient formula in component form for parameter $\\theta_j$.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Component-wise Gradient:**\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=1}^{N} (\\pi^{(i)} - y^{(i)}) \\cdot \\varphi_j^{(i)}$$\n",
        "\n",
        "**Where:**\n",
        "- $\\theta_j$ = the $j$-th parameter\n",
        "- $\\varphi_j^{(i)}$ = the $j$-th feature of the $i$-th sample\n",
        "- $\\pi^{(i)}$ = predicted probability for sample $i$\n",
        "- $y^{(i)}$ = actual label for sample $i$\n",
        "\n",
        "**Interpretation:**\n",
        "- $(\\pi^{(i)} - y^{(i)})$ = prediction error for sample $i$\n",
        "- We weight this error by the feature value $\\varphi_j^{(i)}$\n",
        "- Sum over all samples\n",
        "\n",
        "**Example:** For 3 samples and 2 features:\n",
        "$$\\frac{\\partial J}{\\partial \\theta_1} = (\\pi^{(1)} - y^{(1)})\\varphi_1^{(1)} + (\\pi^{(2)} - y^{(2)})\\varphi_1^{(2)} + (\\pi^{(3)} - y^{(3)})\\varphi_1^{(3)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20. Compare the gradient of logistic regression with the gradient of linear regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Linear Regression Gradient:**\n",
        "$$\\nabla_\\theta J = \\sum_{i=1}^{N} \\varphi^{(i)} \\cdot (\\hat{y}^{(i)} - y^{(i)})$$\n",
        "\n",
        "Where $\\hat{y}^{(i)} = \\varphi^{(i)T}\\theta$\n",
        "\n",
        "**Logistic Regression Gradient:**\n",
        "$$\\nabla_\\theta J = \\sum_{i=1}^{N} \\varphi^{(i)} \\cdot (\\pi^{(i)} - y^{(i)})$$\n",
        "\n",
        "Where $\\pi^{(i)} = s(\\varphi^{(i)T}\\theta)$\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "| Aspect | Linear Regression | Logistic Regression |\n",
        "|--------|-------------------|---------------------|\n",
        "| Form | $\\varphi \\cdot (\\hat{y} - y)$ | $\\varphi \\cdot (\\pi - y)$ |\n",
        "| Prediction | $\\hat{y} = \\varphi^T\\theta$ | $\\pi = s(\\varphi^T\\theta)$ |\n",
        "| Range of prediction | $(-\\infty, +\\infty)$ | $(0, 1)$ |\n",
        "| Closed-form solution | Yes (Normal Equations) | No |\n",
        "\n",
        "**Key Insight:** The gradients look remarkably similar! The only difference is linear vs sigmoid prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Gradient Descent Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21. Write the gradient descent algorithm for logistic regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Algorithm: Gradient Descent for Logistic Regression**\n",
        "\n",
        "```json\n",
        "Input: Training data {(φ(i), y(i))}_{i=1}^N, learning rate α, max_iterations\n",
        "Output: Optimal parameters θ*\n",
        "\n",
        "1. Initialize θ randomly or with zeros\n",
        "\n",
        "2. Repeat until convergence or max_iterations:\n",
        "   \n",
        "   a. For each sample i, compute:\n",
        "      π(i) = sigmoid(φ(i)^T θ)\n",
        "   \n",
        "   b. Compute gradient:\n",
        "      ∇J = Σ φ(i) · (π(i) - y(i))\n",
        "   \n",
        "   c. Update parameters:\n",
        "      θ_new = θ_old - α · ∇J\n",
        "\n",
        "3. Return θ\n",
        "```\n",
        "\n",
        "**Update Rule:**\n",
        "$$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla_\\theta J(\\theta)$$\n",
        "\n",
        "**Component-wise Update:**\n",
        "$$\\theta_j = \\theta_j - \\alpha \\sum_{i=1}^{N} (\\pi^{(i)} - y^{(i)}) \\cdot \\varphi_j^{(i)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22. Why is there no closed-form solution for logistic regression unlike linear regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Linear Regression:**\n",
        "- Cost: $J(\\theta) = ||\\Phi\\theta - y||^2$\n",
        "- Gradient: $\\nabla J = 2\\Phi^T\\Phi\\theta - 2\\Phi^Ty$\n",
        "- Setting to zero: $\\Phi^T\\Phi\\theta = \\Phi^Ty$\n",
        "- **Closed-form:** $\\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$\n",
        "\n",
        "**Logistic Regression:**\n",
        "- Cost: $J(\\theta) = -\\sum[y\\ln(s(\\varphi^T\\theta)) + (1-y)\\ln(1-s(\\varphi^T\\theta))]$\n",
        "- Gradient: $\\nabla J = \\sum \\varphi(s(\\varphi^T\\theta) - y)$\n",
        "\n",
        "**Why No Closed-Form:**\n",
        "\n",
        "1. **Non-linear function:** Sigmoid is inside the expression\n",
        "2. **Cannot isolate θ:** Setting gradient to zero gives:\n",
        "   $$\\sum \\varphi \\cdot s(\\varphi^T\\theta) = \\sum \\varphi \\cdot y$$\n",
        "3. **Transcendental equation:** $\\theta$ appears inside exponential\n",
        "4. **No algebraic solution:** Cannot solve analytically\n",
        "\n",
        "**Solution:** Use iterative methods like gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23. Explain the effect of learning rate (α) on gradient descent. What happens with too large or too small α?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Learning Rate (α):** Controls the step size in each iteration\n",
        "\n",
        "$$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla J$$\n",
        "\n",
        "**Too Large α:**\n",
        "\n",
        "| Problem | Description |\n",
        "|---------|-------------|\n",
        "| Overshooting | Jumps over the minimum |\n",
        "| Oscillation | Bounces back and forth |\n",
        "| Divergence | Cost keeps increasing |\n",
        "| Instability | Never converges |\n",
        "\n",
        "**Too Small α:**\n",
        "\n",
        "| Problem | Description |\n",
        "|---------|-------------|\n",
        "| Slow convergence | Takes many iterations |\n",
        "| Time consuming | Very slow training |\n",
        "| Local minima | May get stuck (though logistic loss is convex) |\n",
        "| Practical issues | May never reach minimum in reasonable time |\n",
        "\n",
        "**Ideal α:**\n",
        "- Fast but stable convergence\n",
        "- Cost decreases smoothly\n",
        "- Reaches minimum in reasonable time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24. What is the convergence criterion for gradient descent in logistic regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Common Convergence Criteria:**\n",
        "\n",
        "1. **Small gradient:**\n",
        "   $$||\\nabla J(\\theta)|| < \\epsilon$$\n",
        "   Stop when gradient magnitude is very small\n",
        "\n",
        "2. **Small change in cost:**\n",
        "   $$|J(\\theta_{new}) - J(\\theta_{old})| < \\epsilon$$\n",
        "   Stop when cost barely changes\n",
        "\n",
        "3. **Small change in parameters:**\n",
        "   $$||\\theta_{new} - \\theta_{old}|| < \\epsilon$$\n",
        "   Stop when parameters barely change\n",
        "\n",
        "4. **Maximum iterations:**\n",
        "   $$\\text{iterations} > \\text{max\\_iter}$$\n",
        "   Stop after fixed number of iterations\n",
        "\n",
        "**Typical Values:**\n",
        "- $\\epsilon = 10^{-6}$ or $10^{-8}$\n",
        "- max_iter = 1000 or 10000\n",
        "\n",
        "**Best Practice:** Use combination of criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Numerical Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25. Given a single training sample with $\\varphi = [1, 2]^T$, $y = 1$, and current $\\theta = [0.5, 0.5]^T$, compute one gradient descent update with $\\alpha = 0.1$.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:**\n",
        "- $\\varphi = [1, 2]^T$\n",
        "- $y = 1$\n",
        "- $\\theta = [0.5, 0.5]^T$\n",
        "- $\\alpha = 0.1$\n",
        "\n",
        "**Step 1: Compute linear combination**\n",
        "$$a = \\varphi^T\\theta = 1(0.5) + 2(0.5) = 0.5 + 1.0 = 1.5$$\n",
        "\n",
        "**Step 2: Compute sigmoid**\n",
        "$$\\pi = s(1.5) = \\frac{1}{1 + e^{-1.5}} = \\frac{1}{1 + 0.223} = \\frac{1}{1.223} \\approx 0.818$$\n",
        "\n",
        "**Step 3: Compute error**\n",
        "$$\\pi - y = 0.818 - 1 = -0.182$$\n",
        "\n",
        "**Step 4: Compute gradient**\n",
        "$$\\nabla J = \\varphi \\cdot (\\pi - y) = [1, 2]^T \\cdot (-0.182) = [-0.182, -0.364]^T$$\n",
        "\n",
        "**Step 5: Update parameters**\n",
        "$$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla J$$\n",
        "$$= [0.5, 0.5]^T - 0.1 \\cdot [-0.182, -0.364]^T$$\n",
        "$$= [0.5 + 0.0182, 0.5 + 0.0364]^T$$\n",
        "$$= [0.518, 0.536]^T$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26. For two samples: $(\\varphi_1 = [1, 0]^T, y_1 = 0)$ and $(\\varphi_2 = [0, 1]^T, y_2 = 1)$, with $\\theta = [0, 0]^T$, calculate the total cost $J(\\theta)$.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:**\n",
        "- Sample 1: $\\varphi_1 = [1, 0]^T$, $y_1 = 0$\n",
        "- Sample 2: $\\varphi_2 = [0, 1]^T$, $y_2 = 1$\n",
        "- $\\theta = [0, 0]^T$\n",
        "\n",
        "**For Sample 1:**\n",
        "$$a_1 = \\varphi_1^T\\theta = 1(0) + 0(0) = 0$$\n",
        "$$\\pi_1 = s(0) = 0.5$$\n",
        "$$\\text{Cost}_1 = -[y_1\\ln(\\pi_1) + (1-y_1)\\ln(1-\\pi_1)]$$\n",
        "$$= -[0 \\cdot \\ln(0.5) + 1 \\cdot \\ln(0.5)]$$\n",
        "$$= -\\ln(0.5) = 0.693$$\n",
        "\n",
        "**For Sample 2:**\n",
        "$$a_2 = \\varphi_2^T\\theta = 0(0) + 1(0) = 0$$\n",
        "$$\\pi_2 = s(0) = 0.5$$\n",
        "$$\\text{Cost}_2 = -[1 \\cdot \\ln(0.5) + 0 \\cdot \\ln(0.5)]$$\n",
        "$$= -\\ln(0.5) = 0.693$$\n",
        "\n",
        "**Total Cost:**\n",
        "$$J(\\theta) = \\text{Cost}_1 + \\text{Cost}_2 = 0.693 + 0.693 = 1.386$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27. For the decision boundary $2x_1 + 3x_2 - 1 = 0$, determine which class the point $(1, 1)$ belongs to.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:**\n",
        "- Decision boundary: $2x_1 + 3x_2 - 1 = 0$\n",
        "- Point to classify: $(x_1, x_2) = (1, 1)$\n",
        "\n",
        "**Step 1: Identify θ and φ**\n",
        "$$\\theta = [2, 3, -1]^T \\text{ (including bias)}$$\n",
        "$$\\varphi = [x_1, x_2, 1]^T = [1, 1, 1]^T$$\n",
        "\n",
        "**Step 2: Compute linear combination**\n",
        "$$\\varphi^T\\theta = 2(1) + 3(1) + (-1)(1) = 2 + 3 - 1 = 4$$\n",
        "\n",
        "**Step 3: Apply classification rule**\n",
        "Since $\\varphi^T\\theta = 4 > 0$:\n",
        "- The point is on the positive side of the boundary\n",
        "- **Classification: Class 1**\n",
        "\n",
        "**Verification with probability:**\n",
        "$$\\pi = s(4) = \\frac{1}{1 + e^{-4}} \\approx 0.982$$\n",
        "Since $0.982 > 0.5$, classify as Class 1 ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28. Given that the sigmoid output is 0.73, what is the value of the input $a$?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Given:** $s(a) = 0.73$\n",
        "\n",
        "**To Find:** $a$\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "$$s(a) = \\frac{1}{1 + e^{-a}} = 0.73$$\n",
        "\n",
        "$$1 + e^{-a} = \\frac{1}{0.73} = 1.370$$\n",
        "\n",
        "$$e^{-a} = 1.370 - 1 = 0.370$$\n",
        "\n",
        "Taking natural log:\n",
        "$$-a = \\ln(0.370) = -0.994$$\n",
        "\n",
        "$$a = 0.994 \\approx 1.0$$\n",
        "\n",
        "**Verification:**\n",
        "$$s(1.0) = \\frac{1}{1 + e^{-1}} = \\frac{1}{1 + 0.368} = \\frac{1}{1.368} \\approx 0.731$$ ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Conceptual Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q29. Compare Linear Regression and Logistic Regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect | Linear Regression | Logistic Regression |\n",
        "|--------|-------------------|---------------------|\n",
        "| **Task** | Regression (continuous output) | Classification (discrete output) |\n",
        "| **Output** | Any real number | Probability in $(0, 1)$ |\n",
        "| **Model** | $\\hat{y} = \\varphi^T\\theta$ | $\\pi = s(\\varphi^T\\theta)$ |\n",
        "| **Cost Function** | Mean Squared Error | Binary Cross-Entropy |\n",
        "| **Gradient** | $\\varphi(\\hat{y} - y)$ | $\\varphi(\\pi - y)$ |\n",
        "| **Closed-form** | Yes (Normal Equations) | No |\n",
        "| **Decision Boundary** | N/A | Linear (hyperplane) |\n",
        "| **Example** | Predict house price | Predict spam/not spam |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q30. What does it mean that logistic regression has a \"linear decision boundary\"?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Linear Decision Boundary:**\n",
        "\n",
        "The decision boundary is the set of points where:\n",
        "$$\\varphi^T\\theta = 0$$\n",
        "\n",
        "**In Different Dimensions:**\n",
        "\n",
        "| Dimension | Boundary Shape | Equation |\n",
        "|-----------|----------------|----------|\n",
        "| 2D | Line | $\\theta_1 x_1 + \\theta_2 x_2 + \\theta_0 = 0$ |\n",
        "| 3D | Plane | $\\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_0 = 0$ |\n",
        "| nD | Hyperplane | $\\sum_{j=1}^{n} \\theta_j x_j + \\theta_0 = 0$ |\n",
        "\n",
        "**Implications:**\n",
        "1. Can only separate linearly separable data\n",
        "2. Cannot learn XOR-like patterns\n",
        "3. Simple but limited\n",
        "\n",
        "**Visual Example:**\n",
        "- Cats on one side of the line\n",
        "- Dogs on the other side\n",
        "- The dashed line is the decision boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q31. Can logistic regression handle non-linearly separable data? How?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Basic Logistic Regression:**\n",
        "- Cannot handle non-linearly separable data\n",
        "- Decision boundary is always a hyperplane\n",
        "\n",
        "**Solutions for Non-linear Data:**\n",
        "\n",
        "1. **Feature Engineering:**\n",
        "   - Add polynomial features: $x_1^2, x_2^2, x_1 x_2$\n",
        "   - Original: $\\varphi = [x_1, x_2, 1]^T$\n",
        "   - Extended: $\\varphi = [x_1, x_2, x_1^2, x_2^2, x_1 x_2, 1]^T$\n",
        "\n",
        "2. **Kernel Methods:**\n",
        "   - Transform data to higher dimension\n",
        "   - Linear boundary in high dimension = curved boundary in original space\n",
        "\n",
        "3. **Use Different Algorithms:**\n",
        "   - Neural Networks\n",
        "   - Decision Trees\n",
        "   - SVM with kernels\n",
        "\n",
        "**Example:**\n",
        "For circular boundary, add $x_1^2 + x_2^2$ as a feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q32. What is the probabilistic interpretation of logistic regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Probabilistic Model:**\n",
        "\n",
        "Logistic regression models the probability of class membership:\n",
        "\n",
        "$$P(y = 1 | \\varphi) = \\frac{1}{1 + e^{-\\varphi^T\\theta}}$$\n",
        "\n",
        "$$P(y = 0 | \\varphi) = 1 - P(y = 1 | \\varphi) = \\frac{e^{-\\varphi^T\\theta}}{1 + e^{-\\varphi^T\\theta}}$$\n",
        "\n",
        "**Log-Odds (Logit):**\n",
        "\n",
        "$$\\ln\\left(\\frac{P(y=1)}{P(y=0)}\\right) = \\ln\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\varphi^T\\theta$$\n",
        "\n",
        "**Interpretation:**\n",
        "- The linear combination $\\varphi^T\\theta$ models the log-odds\n",
        "- Sigmoid converts log-odds to probability\n",
        "- Each $\\theta_j$ represents change in log-odds when $\\varphi_j$ increases by 1\n",
        "\n",
        "**Maximum Likelihood:**\n",
        "- Cross-entropy loss = negative log-likelihood\n",
        "- Minimizing cross-entropy = maximizing likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q33. How can you adjust the decision threshold based on application requirements?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Default Threshold:** 0.5\n",
        "- Classify as Class 1 if $P(y=1) \\geq 0.5$\n",
        "\n",
        "**When to Adjust:**\n",
        "\n",
        "| Scenario | Threshold | Reason |\n",
        "|----------|-----------|--------|\n",
        "| High cost of FN | Lower (e.g., 0.3) | Don't miss positive cases |\n",
        "| High cost of FP | Higher (e.g., 0.7) | Be more certain before predicting positive |\n",
        "| Imbalanced data | Adjust based on class ratio | Handle class imbalance |\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Disease Detection:** Use threshold 0.3\n",
        "   - Missing a disease (FN) is very costly\n",
        "   - Better to have false alarms than miss cases\n",
        "\n",
        "2. **Spam Detection:** Use threshold 0.7\n",
        "   - Blocking important email (FP) is very costly\n",
        "   - Better to let some spam through\n",
        "\n",
        "**Method:** Use ROC curve to find optimal threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q34. What happens when the data is perfectly linearly separable?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Problem with Perfect Separability:**\n",
        "\n",
        "When data is perfectly separable:\n",
        "\n",
        "1. **Parameters diverge to infinity:**\n",
        "   - $||\\theta|| \\rightarrow \\infty$\n",
        "   - Gradient descent keeps increasing weights\n",
        "\n",
        "2. **Probabilities become extreme:**\n",
        "   - All Class 1 samples: $\\pi \\rightarrow 1$\n",
        "   - All Class 0 samples: $\\pi \\rightarrow 0$\n",
        "\n",
        "3. **Numerical issues:**\n",
        "   - Overflow/underflow in computations\n",
        "   - $\\ln(0)$ or $\\ln(1)$ problems\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "1. **Regularization:**\n",
        "   - Add penalty term: $J(\\theta) + \\lambda||\\theta||^2$\n",
        "   - Prevents parameters from growing too large\n",
        "\n",
        "2. **Early stopping:**\n",
        "   - Stop training before divergence\n",
        "\n",
        "3. **Maximum iterations:**\n",
        "   - Limit number of gradient descent steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Extension Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q35. How can logistic regression be extended to multi-class classification?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Two Main Approaches:**\n",
        "\n",
        "**1. One-vs-All (OvA):**\n",
        "- Train K binary classifiers (one for each class)\n",
        "- Classifier k: Class k vs all other classes\n",
        "- Prediction: Choose class with highest probability\n",
        "\n",
        "**2. Softmax Regression (Multinomial Logistic):**\n",
        "\n",
        "$$P(y = k | \\varphi) = \\frac{e^{\\varphi^T\\theta_k}}{\\sum_{j=1}^{K} e^{\\varphi^T\\theta_j}}$$\n",
        "\n",
        "- One parameter vector for each class\n",
        "- Probabilities sum to 1\n",
        "- Natural extension of sigmoid to multiple classes\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "| Aspect | One-vs-All | Softmax |\n",
        "|--------|------------|--------|\n",
        "| Classifiers | K separate | 1 joint |\n",
        "| Training | Independent | Coupled |\n",
        "| Probabilities | May not sum to 1 | Sum to 1 exactly |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q36. What is regularization in logistic regression and why is it needed?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Regularization:**\n",
        "Adding a penalty term to the cost function to prevent overfitting.\n",
        "\n",
        "**L2 Regularization (Ridge):**\n",
        "$$J(\\theta) = -\\sum_{i=1}^{N}[y^{(i)}\\ln(\\pi^{(i)}) + (1-y^{(i)})\\ln(1-\\pi^{(i)})] + \\lambda\\sum_{j=1}^{d}\\theta_j^2$$\n",
        "\n",
        "**L1 Regularization (Lasso):**\n",
        "$$J(\\theta) = -\\sum_{i=1}^{N}[y^{(i)}\\ln(\\pi^{(i)}) + (1-y^{(i)})\\ln(1-\\pi^{(i)})] + \\lambda\\sum_{j=1}^{d}|\\theta_j|$$\n",
        "\n",
        "**Why Needed:**\n",
        "\n",
        "1. **Prevents overfitting:** Keeps parameters small\n",
        "2. **Handles perfect separability:** Prevents divergence\n",
        "3. **Feature selection:** L1 can make some weights zero\n",
        "4. **Generalization:** Better performance on test data\n",
        "\n",
        "**λ (Regularization strength):**\n",
        "- Large λ: More regularization, simpler model\n",
        "- Small λ: Less regularization, complex model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q37. Summarize the key points of logistic regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Key Points Summary:**\n",
        "\n",
        "**1. Purpose:**\n",
        "- Binary classification (can extend to multi-class)\n",
        "- Estimates probability of class membership\n",
        "\n",
        "**2. Model:**\n",
        "$$P(y=1|\\varphi) = \\frac{1}{1 + e^{-\\varphi^T\\theta}}$$\n",
        "\n",
        "**3. Cost Function:**\n",
        "$$J(\\theta) = -\\sum_{i=1}^{N}[y^{(i)}\\ln(\\pi^{(i)}) + (1-y^{(i)})\\ln(1-\\pi^{(i)})]$$\n",
        "\n",
        "**4. Gradient:**\n",
        "$$\\nabla J = \\sum_{i=1}^{N}\\varphi^{(i)}(\\pi^{(i)} - y^{(i)})$$\n",
        "\n",
        "**5. Optimization:**\n",
        "- No closed-form solution\n",
        "- Use gradient descent: $\\theta_{new} = \\theta_{old} - \\alpha \\nabla J$\n",
        "\n",
        "**6. Decision Boundary:**\n",
        "- Linear (hyperplane)\n",
        "- $\\varphi^T\\theta = 0$\n",
        "\n",
        "**7. Properties:**\n",
        "- Convex cost function\n",
        "- Unique global minimum\n",
        "- Simple and interpretable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary of Important Formulas\n",
        "\n",
        "| Concept | Formula |\n",
        "|---------|--------|\n",
        "| Sigmoid Function | $s(a) = \\frac{1}{1 + e^{-a}}$ |\n",
        "| Sigmoid Property | $s(-a) = 1 - s(a)$ |\n",
        "| Sigmoid Derivative | $\\frac{ds}{da} = s(a)(1 - s(a))$ |\n",
        "| Probability Model | $P(y=1|\\varphi) = s(\\varphi^T\\theta)$ |\n",
        "| Log Loss (single) | $f = -[y\\ln(\\pi) + (1-y)\\ln(1-\\pi)]$ |\n",
        "| Cost Function | $J(\\theta) = -\\sum_{i=1}^{N}[y^{(i)}\\ln(\\pi^{(i)}) + (1-y^{(i)})\\ln(1-\\pi^{(i)})]$ |\n",
        "| Gradient | $\\nabla_\\theta J = \\sum_{i=1}^{N}\\varphi^{(i)}(\\pi^{(i)} - y^{(i)})$ |\n",
        "| Update Rule | $\\theta_{new} = \\theta_{old} - \\alpha \\nabla J$ |\n",
        "| Decision Boundary | $\\varphi^T\\theta = 0$ |\n",
        "| Classification Rule | Class 1 if $\\varphi^T\\theta \\geq 0$, else Class 0 |\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
